{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create base graph by walking through the directory and finding files and dirs, adding them to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "from edoc.connect import connect_to_neo4j\n",
    "\n",
    "class CodebaseGraph:\n",
    "    def __init__(self, root_directory, uri=\"bolt://localhost:7687\", user=None, password=None, openai_api_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the CodebaseGraph with a connection to Neo4j.\n",
    "\n",
    "        Args:\n",
    "            uri (str): The URI of the Neo4j database. Defaults to localhost.\n",
    "            user (str): Username for Neo4j. If None, loads from environment variable NEO4J_USERNAME.\n",
    "            password (str): Password for Neo4j. If None, loads from environment variable NEO4J_PASSWORD.\n",
    "            openai_api_key (str): Key needed to access OpenAI API\n",
    "        \"\"\"\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "\n",
    "        # Set up the Neo4j connection\n",
    "        self.uri = uri\n",
    "        self.NEO4J_USER = user or os.getenv(\"NEO4J_USERNAME\")\n",
    "        self.NEO4J_PASSWORD =  password or os.getenv(\"NEO4J_PASSWORD\")\n",
    "        self.OPENAI_API_KEY = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        self.root_directory = root_directory\n",
    "\n",
    "        if not self.NEO4J_USER or not self.NEO4J_PASSWORD:\n",
    "            raise ValueError(\"NEO4J_USERNAME and NEO4J_PASSWORD must be provided either as arguments or environment variables.\")\n",
    "        \n",
    "        if not self.OPENAI_API_KEY:\n",
    "            raise ValueError(\"NEO4J_USERNAME and NEO4J_PASSWORD must be provided either as arguments or environment variables.\")\n",
    "\n",
    "        self.kg = connect_to_neo4j()\n",
    "\n",
    "    def get_file_info(self, file_path):\n",
    "        \"\"\"\n",
    "        Get information about a file, including type, size, last modified date, creation date, permissions, owner, and hash.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing file information.\n",
    "        \"\"\"\n",
    "        stats = os.stat(file_path)\n",
    "        file_type = os.path.splitext(file_path)[1][1:]  # Get file extension without the dot\n",
    "        size = stats.st_size\n",
    "        last_modified = datetime.fromtimestamp(stats.st_mtime).isoformat()\n",
    "        created = datetime.fromtimestamp(stats.st_ctime).isoformat()\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"type\": file_type,\n",
    "            \"size\": size,\n",
    "            \"last_modified\": last_modified,\n",
    "            \"created\": created,\n",
    "        }\n",
    "\n",
    "    def _load_dirs_and_files_to_graph(self):\n",
    "        \"\"\"\n",
    "        Traverse a directory and create a graph in Neo4j representing the directory structure and file information.\n",
    "\n",
    "        Args:\n",
    "            root_directory (str): The root directory to start traversing from.\n",
    "        \"\"\"\n",
    "        for root, dirs, files in os.walk(self.root_directory):\n",
    "\n",
    "            dir_name = os.path.basename(root)\n",
    "\n",
    "            # Create node for the directory\n",
    "            self.kg.query(\n",
    "                \"\"\"\n",
    "                MERGE (dir:Directory {name: $dir_name, path: $path})\n",
    "                ON CREATE SET dir.created = $created, dir.last_modified = $last_modified\n",
    "                \"\"\",\n",
    "                {\n",
    "                    'dir_name':dir_name,\n",
    "                    'path':root,\n",
    "                    'created':datetime.fromtimestamp(os.stat(root).st_ctime).isoformat(),\n",
    "                    'last_modified':datetime.fromtimestamp(os.stat(root).st_mtime).isoformat(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Create nodes for subdirectories\n",
    "            for dir_name in dirs:\n",
    "                dir_path = os.path.join(root, dir_name)\n",
    "                self.kg.query(\n",
    "                    \"\"\"\n",
    "                        MERGE (subdir:Directory {name: $dir_name, path: $subdir_path})\n",
    "                        ON CREATE SET subdir.created = $created, subdir.last_modified = $last_modified\n",
    "                        WITH subdir\n",
    "                        MATCH (parent:Directory {path: $parent_path})\n",
    "                        MERGE (parent)-[:CONTAINS]->(subdir)\n",
    "                    \"\"\",\n",
    "                    {\n",
    "                        'dir_name':dir_name,\n",
    "                        'subdir_path':dir_path, \n",
    "                        'parent_path':root,\n",
    "                        'created':datetime.fromtimestamp(os.stat(dir_path).st_ctime).isoformat(),\n",
    "                        'last_modified':datetime.fromtimestamp(os.stat(dir_path).st_mtime).isoformat(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Create nodes for files\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                file_info = self.get_file_info(file_path)\n",
    "                self.kg.query(\n",
    "                    \"\"\"\n",
    "                        MERGE (file:File {name: $file_name, path: $file_path})\n",
    "                        ON CREATE SET file.type = $type, file.size = $size, file.last_modified = $last_modified, file.created = $created\n",
    "                        WITH file\n",
    "                        MATCH (parent:Directory {path: $parent_path})\n",
    "                        MERGE (parent)-[:CONTAINS]->(file)\n",
    "                    \"\"\",\n",
    "                    {\n",
    "                        'file_name':file_name, \n",
    "                        'file_path':file_path, \n",
    "                        'type':file_info['type'], \n",
    "                        'size':file_info['size'], \n",
    "                        'last_modified':file_info['last_modified'], \n",
    "                        'created':file_info['created'], \n",
    "                        'parent_path':root\n",
    "                    }\n",
    "                )\n",
    "        \n",
    "    def create_graph(self):\n",
    "\n",
    "        self._load_dirs_and_files_to_graph()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next work on file enrichments. We will need to create functions to connect to opneai api as well as functions to perform embeddings and extractoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "def create_chat_completion(messages, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Create a chat completion using the OpenAI API.\n",
    "\n",
    "    Args:\n",
    "        messages (list): A list of message dictionaries for the conversation.\n",
    "        model (str): The OpenAI model to use. Default is 'gpt-4o-mini'.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the response from the OpenAI API.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    response = client.chat.completions.create(messages=messages, model=model)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def summarize_file_chunk(chunk_text, file_name, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Summarize a chunk of text from a file using OpenAI's language model.\n",
    "\n",
    "    Args:\n",
    "        chunk_text (str): The text chunk to summarize.\n",
    "        file_name (str): The name of the file from which the chunk was extracted.\n",
    "        model (str): The OpenAI model to use. Default is 'gpt-4o-mini'.\n",
    "\n",
    "    Returns:\n",
    "        str: A brief and clear summary of the chunk.\n",
    "    \"\"\"\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"I have this text from a file named {file_name}. The text is:\\n{chunk_text}\\nPlease summarize it in simple terms. Try to keep the summary brief, but maintain clarity.\"}\n",
    "    ]\n",
    "    return create_chat_completion(messages=prompt, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = \"utils.py\"\n",
    "test_chunk = \"\"\"\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def process_file(input_path, output_path):\n",
    "    with open(input_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(content.upper())\n",
    "\n",
    "    print(f\"Processed {{input_path}} and saved results to {{output_path}}\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_summary = summarize_file_chunk(file_name=test_file_name, chunk_text=test_chunk)\n",
    "print(test_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Up next we will create embeddings from a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Generate an embedding for a given text using OpenAI's embedding model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be embedded. Newlines are replaced with spaces.\n",
    "        model (str): The OpenAI model to use. Default is 'text-embedding-3-small'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of floats representing the embedding vector of the input text.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding = get_embedding(text=test_summary)\n",
    "len(test_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onto creating a structured output to use with llms. The goal should be to take code and extract imports, functions, and classes. We use [this](https://medium.com/neo4j/enhancing-the-accuracy-of-rag-applications-with-knowledge-graphs-ad5e2ffab663) for insperation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm=ChatOpenAI(\n",
    "    model_name='gpt-4o-mini'\n",
    ")\n",
    "\n",
    "class Parameter(BaseModel):\n",
    "    \"\"\"Model representing a function or class parameter.\"\"\"\n",
    "    name: str\n",
    "    type: str\n",
    "\n",
    "class FunctionEntity(BaseModel):\n",
    "    \"\"\"Model representing a function entity.\"\"\"\n",
    "    name: str\n",
    "    parameters: List[Parameter]\n",
    "    return_type: Optional[str] = None\n",
    "\n",
    "class ClassEntity(BaseModel):\n",
    "    \"\"\"Model representing a class entity.\"\"\"\n",
    "    name: str\n",
    "    parameters: List[Parameter]\n",
    "\n",
    "class ImportEntity(BaseModel):\n",
    "    \"\"\"Model representing an import entity.\"\"\"\n",
    "    module: str\n",
    "    entities: List[str]\n",
    "\n",
    "class CodeEntities(BaseModel):\n",
    "    \"\"\"Identifying information about code entities.\"\"\"\n",
    "    \n",
    "    imports: List[ImportEntity] = Field(\n",
    "        default=[],\n",
    "        description=\"All the import statements in the code, with the module \"\n",
    "        \"and specific entities being imported.\",\n",
    "    )\n",
    "    functions: List[FunctionEntity] = Field(\n",
    "        default=[],\n",
    "        description=\"All the function names in the code, including their parameters, returns, \"\n",
    "        \"and types if available.\",\n",
    "    )\n",
    "    classes: List[ClassEntity] = Field(\n",
    "        default=[],\n",
    "        description=\"All the class names in the code, including their parameters \"\n",
    "        \"and types if available.\",\n",
    "    )\n",
    "\n",
    "def extract_code_entities(code_string):\n",
    "    \"\"\"\n",
    "    Extracts code entities from a given code string, including imports, function names, and class names.\n",
    "\n",
    "    This function uses a language model to analyze the provided code string and extract relevant code entities. \n",
    "    For imports, it identifies the module and specific entities being imported. For functions and classes, it \n",
    "    captures their names, parameters, and types if available.\n",
    "\n",
    "    Args:\n",
    "        code_string (str): The code snippet as a string from which to extract entities.\n",
    "\n",
    "    Returns:\n",
    "        entities: An instance of CodeEntities containing the extracted imports, functions, and classes.\n",
    "    \"\"\"\n",
    "    # Modify the prompt to focus on extracting code entities\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are extracting imports, function names, and class names from the given code. \"\n",
    "                \"For imports, provide the module and specific entities being imported. \"\n",
    "                \"For functions and classes, include their parameters and types if available.\",\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Use the given format to extract information from the following input: {code_snippet}\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Set up the chain to extract the structured output\n",
    "    entity_chain = prompt | llm.with_structured_output(CodeEntities)\n",
    "\n",
    "    entities = entity_chain.invoke({'code_snippet': code_string})\n",
    "\n",
    "    entities = entities.dict()\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_example = \"\"\"\n",
    "import java.util.List;\n",
    "import java.util.Optional;\n",
    "\n",
    "public class ExampleClass {\n",
    "    private String param1;\n",
    "    private int param2;\n",
    "\n",
    "    public ExampleClass(String param1, int param2) {\n",
    "        this.param1 = param1;\n",
    "        this.param2 = param2;\n",
    "    }\n",
    "\n",
    "    public boolean exampleMethod(List<String> arg1, Optional<Integer> arg2) {\n",
    "        return arg2.isPresent();\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_entities = extract_code_entities(entity_example)\n",
    "\n",
    "test_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now prepare to chunk files to be iterated through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "\n",
    "text_splitter = PythonCodeTextSplitter(chunk_size=500, chunk_overlap=25)\n",
    "chunks = text_splitter.split_text(entity_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_contents(file_path):\n",
    "    \"\"\"\n",
    "    Opens a file and reads its contents as text.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to be read.\n",
    "\n",
    "    Returns:\n",
    "        str: The contents of the file as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            contents = file.read()\n",
    "        return contents\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "from edoc.connect import connect_to_neo4j\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "\n",
    "\n",
    "class CodebaseGraph:\n",
    "    def __init__(\n",
    "            self, \n",
    "            root_directory, \n",
    "            uri=\"bolt://localhost:7687\", \n",
    "            user=None, \n",
    "            password=None, \n",
    "            openai_api_key=None,\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=25\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the CodebaseGraph with a connection to Neo4j.\n",
    "\n",
    "        Args:\n",
    "            uri (str): The URI of the Neo4j database. Defaults to localhost.\n",
    "            user (str): Username for Neo4j. If None, loads from environment variable NEO4J_USERNAME.\n",
    "            password (str): Password for Neo4j. If None, loads from environment variable NEO4J_PASSWORD.\n",
    "            openai_api_key (str): Key needed to access OpenAI API\n",
    "        \"\"\"\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "\n",
    "        # Set up the Neo4j connection\n",
    "        self.uri = uri\n",
    "        self.NEO4J_USER = user or os.getenv(\"NEO4J_USERNAME\")\n",
    "        self.NEO4J_PASSWORD =  password or os.getenv(\"NEO4J_PASSWORD\")\n",
    "        self.OPENAI_API_KEY = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        self.root_directory = root_directory\n",
    "\n",
    "        if not self.NEO4J_USER or not self.NEO4J_PASSWORD:\n",
    "            raise ValueError(\"NEO4J_USERNAME and NEO4J_PASSWORD must be provided either as arguments or environment variables.\")\n",
    "        \n",
    "        if not self.OPENAI_API_KEY:\n",
    "            raise ValueError(\"NEO4J_USERNAME and NEO4J_PASSWORD must be provided either as arguments or environment variables.\")\n",
    "\n",
    "        self.kg = connect_to_neo4j()\n",
    "\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "\n",
    "    def get_file_info(self, file_path):\n",
    "        \"\"\"\n",
    "        Get information about a file, including type, size, last modified date, creation date, permissions, owner, and hash.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing file information.\n",
    "        \"\"\"\n",
    "        stats = os.stat(file_path)\n",
    "        file_type = os.path.splitext(file_path)[1][1:]  # Get file extension without the dot\n",
    "        size = stats.st_size\n",
    "        last_modified = datetime.fromtimestamp(stats.st_mtime).isoformat()\n",
    "        created = datetime.fromtimestamp(stats.st_ctime).isoformat()\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"type\": file_type,\n",
    "            \"size\": size,\n",
    "            \"last_modified\": last_modified,\n",
    "            \"created\": created,\n",
    "        }\n",
    "    \n",
    "    def read_file_contents(self, file_path):\n",
    "        \"\"\"\n",
    "        Opens a file and reads its contents as text.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file to be read.\n",
    "\n",
    "        Returns:\n",
    "            str: The contents of the file as a string.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                contents = file.read()\n",
    "            return contents\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading the file: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _load_dirs_and_files_to_graph(self):\n",
    "        \"\"\"\n",
    "        Traverse a directory and create a graph in Neo4j representing the directory structure and file information.\n",
    "\n",
    "        Args:\n",
    "            root_directory (str): The root directory to start traversing from.\n",
    "        \"\"\"\n",
    "        for root, dirs, files in os.walk(self.root_directory):\n",
    "\n",
    "            dir_name = os.path.basename(root)\n",
    "\n",
    "            # Create node for the directory\n",
    "            self.kg.query(\n",
    "                \"\"\"\n",
    "                MERGE (dir:Directory {name: $dir_name, path: $path})\n",
    "                ON CREATE SET dir.created = $created, dir.last_modified = $last_modified\n",
    "                \"\"\",\n",
    "                {\n",
    "                    'dir_name':dir_name,\n",
    "                    'path':root,\n",
    "                    'created':datetime.fromtimestamp(os.stat(root).st_ctime).isoformat(),\n",
    "                    'last_modified':datetime.fromtimestamp(os.stat(root).st_mtime).isoformat(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Create nodes for subdirectories\n",
    "            for dir_name in dirs:\n",
    "                dir_path = os.path.join(root, dir_name)\n",
    "                self.kg.query(\n",
    "                    \"\"\"\n",
    "                        MERGE (subdir:Directory {name: $dir_name, path: $subdir_path})\n",
    "                        ON CREATE SET subdir.created = $created, subdir.last_modified = $last_modified\n",
    "                        WITH subdir\n",
    "                        MATCH (parent:Directory {path: $parent_path})\n",
    "                        MERGE (parent)-[:CONTAINS]->(subdir)\n",
    "                    \"\"\",\n",
    "                    {\n",
    "                        'dir_name':dir_name,\n",
    "                        'subdir_path':dir_path, \n",
    "                        'parent_path':root,\n",
    "                        'created':datetime.fromtimestamp(os.stat(dir_path).st_ctime).isoformat(),\n",
    "                        'last_modified':datetime.fromtimestamp(os.stat(dir_path).st_mtime).isoformat(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Create nodes for files\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                file_info = self.get_file_info(file_path)\n",
    "                self.kg.query(\n",
    "                    \"\"\"\n",
    "                        MERGE (file:File {name: $file_name, path: $file_path})\n",
    "                        ON CREATE SET file.type = $type, file.size = $size, file.last_modified = $last_modified, file.created = $created\n",
    "                        WITH file\n",
    "                        MATCH (parent:Directory {path: $parent_path})\n",
    "                        MERGE (parent)-[:CONTAINS]->(file)\n",
    "                    \"\"\",\n",
    "                    {\n",
    "                        'file_name':file_name, \n",
    "                        'file_path':file_path, \n",
    "                        'type':file_info['type'], \n",
    "                        'size':file_info['size'], \n",
    "                        'last_modified':file_info['last_modified'], \n",
    "                        'created':file_info['created'], \n",
    "                        'parent_path':root\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    def _enrich_graph(self):\n",
    "        \"\"\"\n",
    "        Enriches the knowledge graph by processing files, creating and linking code chunks, and extracting unique code entities.\n",
    "        \"\"\"\n",
    "        text_splitter = PythonCodeTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "\n",
    "        # Query to find files without chunk nodes\n",
    "        query = \"\"\"\n",
    "        MATCH (file:File)\n",
    "        WHERE NOT (file)-[:CONTAINS]->(:Chunk)\n",
    "        RETURN file.path AS file_path\n",
    "        \"\"\"\n",
    "\n",
    "        result = self.kg.query(query)\n",
    "        file_paths = [record['file_path'] for record in result]\n",
    "\n",
    "        for file in tqdm(file_paths):\n",
    "            file_contents = self.read_file_contents(file)\n",
    "\n",
    "            if file_contents is not None:\n",
    "                chunks = text_splitter.split_text(file_contents)\n",
    "\n",
    "                # Initialize collections for unique entities\n",
    "                unique_imports = {}\n",
    "                unique_functions = {}\n",
    "                unique_classes = {}\n",
    "\n",
    "                for idx, chunk in enumerate(chunks):\n",
    "                    chunk_id = f\"{file}_chunk_{idx}\"\n",
    "                    chunk_summary = summarize_file_chunk(chunk_text=chunk, file_name=file)\n",
    "                    summary_embedding = get_embedding(chunk_summary)\n",
    "                    chunk_embedding = get_embedding(chunk)\n",
    "\n",
    "                    # Create the chunk node and link it to the file\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (chunk:Chunk {id: $chunk_id})\n",
    "                        SET chunk.raw_code = $raw_code, \n",
    "                            chunk.summary = $summary, \n",
    "                            chunk.summary_embedding = $summary_embedding, \n",
    "                            chunk.chunk_embedding = $chunk_embedding\n",
    "                        WITH chunk\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:CONTAINS]->(chunk)\n",
    "                    \"\"\", {\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'raw_code': chunk,\n",
    "                        'summary': chunk_summary,\n",
    "                        'summary_embedding': summary_embedding,\n",
    "                        'chunk_embedding': chunk_embedding,\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                    # Extract and process code entities for each chunk\n",
    "                    chunk_entities = extract_code_entities(chunk)\n",
    "\n",
    "                    # Collect unique imports\n",
    "                    for imp in chunk_entities['imports']:\n",
    "                        module_name = imp['module']\n",
    "                        if module_name not in unique_imports:\n",
    "                            unique_imports[module_name] = set(imp['entities'])\n",
    "                        else:\n",
    "                            unique_imports[module_name].update(imp['entities'])\n",
    "\n",
    "                    # Collect unique functions\n",
    "                    for func in chunk_entities['functions']:\n",
    "                        func_name = func['name']\n",
    "                        if func_name not in unique_functions:\n",
    "                            unique_functions[func_name] = {\n",
    "                                'parameters': json.dumps([{'name': param['name'], 'type': param['type']} for param in func['parameters']]),\n",
    "                                'return_type': func['return_type']\n",
    "                            }\n",
    "\n",
    "                    # Collect unique classes\n",
    "                    for cls in chunk_entities['classes']:\n",
    "                        cls_name = cls['name']\n",
    "                        if cls_name not in unique_classes:\n",
    "                            unique_classes[cls_name] = {\n",
    "                                'parameters': json.dumps([{'name': param['name'], 'type': param['type']} for param in cls['parameters']])\n",
    "                            }\n",
    "\n",
    "                # Store unique entities in the graph\n",
    "\n",
    "                # Create and link unique import nodes\n",
    "                for module, entities in unique_imports.items():\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (import:Import {module: $module})\n",
    "                        SET import.entities = $entities\n",
    "                        WITH import\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:CALLS]->(import)\n",
    "                    \"\"\", {\n",
    "                        'module': module,\n",
    "                        'entities': list(entities),\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                # Create and link unique function nodes\n",
    "                for name, func in unique_functions.items():\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (function:Function {name: $name})\n",
    "                        SET function.parameters = $parameters, function.return_type = $return_type\n",
    "                        WITH function\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:DEFINES]->(function)\n",
    "                    \"\"\", {\n",
    "                        'name': name,\n",
    "                        'parameters': func['parameters'],\n",
    "                        'return_type': func['return_type'],\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                # Create and link unique class nodes\n",
    "                for name, cls in unique_classes.items():\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (class:Class {name: $name})\n",
    "                        SET class.parameters = $parameters\n",
    "                        WITH class\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:DEFINES]->(class)\n",
    "                    \"\"\", {\n",
    "                        'name': name,\n",
    "                        'parameters': cls['parameters'],\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                # Link all chunks in sequence using APOC's `NEXT` relationship\n",
    "                self.kg.query(\"\"\"\n",
    "                    MATCH (file:File {path: $file_path})-[:CONTAINS]->(chunk:Chunk)\n",
    "                    WITH chunk ORDER BY chunk.id ASC\n",
    "                    WITH collect(chunk) AS chunks\n",
    "                    CALL apoc.nodes.link(chunks, 'NEXT')\n",
    "                    RETURN count(*)\n",
    "                \"\"\", {\n",
    "                    'file_path': file\n",
    "                })\n",
    "\n",
    "\n",
    "    def create_graph(self):\n",
    "\n",
    "        self._load_dirs_and_files_to_graph()\n",
    "\n",
    "        self._enrich_graph()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final step will be to go through the graph and look at files with summaries, and then use those summaries to summarize files. This aggregation procecss can be used for all files to summarize directories. After this we need to create vector index for the code snippets and the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_list_of_summaries(summaries, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Summarize a chunk of text from a file using OpenAI's language model.\n",
    "\n",
    "    Args:\n",
    "        summaries (lst[str]): The text chunk to summarize.\n",
    "        model (str): The OpenAI model to use. Default is 'gpt-4o-mini'.\n",
    "\n",
    "    Returns:\n",
    "        str: A brief and clear summary of the chunk.\n",
    "    \"\"\"\n",
    "    context = '\\n'.join(summaries)\n",
    "\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"\"\"I would like you to summarize some information for me. The goal is to condense understanding across a file system.\n",
    "            It may include chunk summaries (which are smaller sections of a file), file summaries, or subdirectory summareis. \n",
    "            The goal is to gain global understanding by merging themes at higher levels.\n",
    "            Please use the given context to make a summary, no need to confirm. Simply return simple and brief summaries based on context. The context is: {context}.\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    return create_chat_completion(messages=prompt, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "from edoc.connect import connect_to_neo4j\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "\n",
    "\n",
    "class CodebaseGraph:\n",
    "    def __init__(\n",
    "            self, \n",
    "            root_directory, \n",
    "            uri=\"bolt://localhost:7687\", \n",
    "            user=None, \n",
    "            password=None, \n",
    "            openai_api_key=None,\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=25\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the CodebaseGraph with a connection to Neo4j.\n",
    "\n",
    "        Args:\n",
    "            uri (str): The URI of the Neo4j database. Defaults to localhost.\n",
    "            user (str): Username for Neo4j. If None, loads from environment variable NEO4J_USERNAME.\n",
    "            password (str): Password for Neo4j. If None, loads from environment variable NEO4J_PASSWORD.\n",
    "            openai_api_key (str): Key needed to access OpenAI API\n",
    "        \"\"\"\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "\n",
    "        # Set up the Neo4j connection\n",
    "        self.uri = uri\n",
    "        self.NEO4J_USER = user or os.getenv(\"NEO4J_USERNAME\")\n",
    "        self.NEO4J_PASSWORD =  password or os.getenv(\"NEO4J_PASSWORD\")\n",
    "        self.OPENAI_API_KEY = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        self.root_directory = root_directory\n",
    "\n",
    "        if not self.NEO4J_USER or not self.NEO4J_PASSWORD:\n",
    "            raise ValueError(\"NEO4J_USERNAME and NEO4J_PASSWORD must be provided either as arguments or environment variables.\")\n",
    "        \n",
    "        if not self.OPENAI_API_KEY:\n",
    "            raise ValueError(\"NEO4J_USERNAME and NEO4J_PASSWORD must be provided either as arguments or environment variables.\")\n",
    "\n",
    "        self.kg = connect_to_neo4j()\n",
    "\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "\n",
    "    def get_file_info(self, file_path):\n",
    "        \"\"\"\n",
    "        Get information about a file, including type, size, last modified date, creation date, permissions, owner, and hash.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing file information.\n",
    "        \"\"\"\n",
    "        stats = os.stat(file_path)\n",
    "        file_type = os.path.splitext(file_path)[1][1:]  # Get file extension without the dot\n",
    "        size = stats.st_size\n",
    "        last_modified = datetime.fromtimestamp(stats.st_mtime).isoformat()\n",
    "        created = datetime.fromtimestamp(stats.st_ctime).isoformat()\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"type\": file_type,\n",
    "            \"size\": size,\n",
    "            \"last_modified\": last_modified,\n",
    "            \"created\": created,\n",
    "        }\n",
    "    \n",
    "    def read_file_contents(self, file_path):\n",
    "        \"\"\"\n",
    "        Opens a file and reads its contents as text.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file to be read.\n",
    "\n",
    "        Returns:\n",
    "            str: The contents of the file as a string.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                contents = file.read()\n",
    "            return contents\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading the file: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def _should_skip_file(self, file_path):\n",
    "        \"\"\"\n",
    "        Determine if a file should be skipped based on its type or size.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the file should be skipped, False otherwise.\n",
    "        \"\"\"\n",
    "        # Get the file extension\n",
    "        _, ext = os.path.splitext(file_path)\n",
    "\n",
    "        # Define file types to skip\n",
    "        skip_extensions = {\n",
    "            '.lock', '.png', '.jpg', '.gif', '.pdf', '.zip', '.class', '.o', '.out',\n",
    "            '.md', '.rst', '.csv', '.tsv'\n",
    "        }\n",
    "\n",
    "        # Define directories to skip\n",
    "        skip_directories = {'node_modules', '.git', '.svn'}\n",
    "\n",
    "        # Check if the file extension is in the skip list\n",
    "        if ext.lower() in skip_extensions:\n",
    "            return True\n",
    "\n",
    "        # Check if the file is in a directory that should be skipped\n",
    "        if any(skip_dir in file_path for skip_dir in skip_directories):\n",
    "            return True\n",
    "\n",
    "        # Optionally, skip large files (e.g., > 5MB)\n",
    "        if os.path.getsize(file_path) > 5 * 1024 * 1024:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "    def _load_dirs_and_files_to_graph(self):\n",
    "        \"\"\"\n",
    "        Traverse a directory and create a graph in Neo4j representing the directory structure and file information.\n",
    "\n",
    "        Args:\n",
    "            root_directory (str): The root directory to start traversing from.\n",
    "        \"\"\"\n",
    "        print(\"Creating file and dir nodes from Walk\")\n",
    "        for root, dirs, files in os.walk(self.root_directory):\n",
    "\n",
    "            dir_name = os.path.basename(root)\n",
    "\n",
    "            # Create node for the directory\n",
    "            self.kg.query(\n",
    "                \"\"\"\n",
    "                MERGE (dir:Directory {name: $dir_name, path: $path})\n",
    "                ON CREATE SET dir.created = $created, dir.last_modified = $last_modified\n",
    "                \"\"\",\n",
    "                {\n",
    "                    'dir_name':dir_name,\n",
    "                    'path':root,\n",
    "                    'created':datetime.fromtimestamp(os.stat(root).st_ctime).isoformat(),\n",
    "                    'last_modified':datetime.fromtimestamp(os.stat(root).st_mtime).isoformat(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Create nodes for subdirectories\n",
    "            for dir_name in dirs:\n",
    "                dir_path = os.path.join(root, dir_name)\n",
    "                self.kg.query(\n",
    "                    \"\"\"\n",
    "                        MERGE (subdir:Directory {name: $dir_name, path: $subdir_path})\n",
    "                        ON CREATE SET subdir.created = $created, subdir.last_modified = $last_modified\n",
    "                        WITH subdir\n",
    "                        MATCH (parent:Directory {path: $parent_path})\n",
    "                        MERGE (parent)-[:CONTAINS]->(subdir)\n",
    "                    \"\"\",\n",
    "                    {\n",
    "                        'dir_name':dir_name,\n",
    "                        'subdir_path':dir_path, \n",
    "                        'parent_path':root,\n",
    "                        'created':datetime.fromtimestamp(os.stat(dir_path).st_ctime).isoformat(),\n",
    "                        'last_modified':datetime.fromtimestamp(os.stat(dir_path).st_mtime).isoformat(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Create nodes for files\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                file_info = self.get_file_info(file_path)\n",
    "                self.kg.query(\n",
    "                    \"\"\"\n",
    "                        MERGE (file:File {name: $file_name, path: $file_path})\n",
    "                        ON CREATE SET file.type = $type, file.size = $size, file.last_modified = $last_modified, file.created = $created\n",
    "                        WITH file\n",
    "                        MATCH (parent:Directory {path: $parent_path})\n",
    "                        MERGE (parent)-[:CONTAINS]->(file)\n",
    "                    \"\"\",\n",
    "                    {\n",
    "                        'file_name':file_name, \n",
    "                        'file_path':file_path, \n",
    "                        'type':file_info['type'], \n",
    "                        'size':file_info['size'], \n",
    "                        'last_modified':file_info['last_modified'], \n",
    "                        'created':file_info['created'], \n",
    "                        'parent_path':root\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    def _enrich_graph(self):\n",
    "        \"\"\"\n",
    "        Enriches the knowledge graph by processing files, creating and linking code chunks, and extracting unique code entities.\n",
    "        \"\"\"\n",
    "        text_splitter = PythonCodeTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "\n",
    "        # Query to find files without chunk nodes\n",
    "        query = \"\"\"\n",
    "        MATCH (file:File)\n",
    "        WHERE NOT (file)-[:CONTAINS]->(:Chunk)\n",
    "        RETURN file.path AS file_path\n",
    "        \"\"\"\n",
    "\n",
    "        result = self.kg.query(query)\n",
    "        file_paths = [record['file_path'] for record in result]\n",
    "\n",
    "        file_paths = [file for file in file_paths if not self._should_skip_file(file)]\n",
    "\n",
    "        for file in tqdm(file_paths, desc='Creating chunks from files'):\n",
    "            file_contents = self.read_file_contents(file)\n",
    "\n",
    "            if file_contents is not None:\n",
    "                chunks = text_splitter.split_text(file_contents)\n",
    "\n",
    "                # Initialize collections for unique entities\n",
    "                unique_imports = {}\n",
    "                unique_functions = {}\n",
    "                unique_classes = {}\n",
    "\n",
    "                for idx, chunk in enumerate(chunks):\n",
    "                    chunk_id = f\"{file}_chunk_{idx:06d}\"\n",
    "                    chunk_summary = summarize_file_chunk(chunk_text=chunk, file_name=file)\n",
    "                    summary_embedding = get_embedding(chunk_summary)\n",
    "                    chunk_embedding = get_embedding(chunk)\n",
    "\n",
    "                    # Create the chunk node and link it to the file\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (chunk:Chunk {id: $chunk_id})\n",
    "                        SET chunk.raw_code = $raw_code, \n",
    "                            chunk.summary = $summary, \n",
    "                            chunk.summary_embedding = $summary_embedding, \n",
    "                            chunk.chunk_embedding = $chunk_embedding\n",
    "                        WITH chunk\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:CONTAINS]->(chunk)\n",
    "                    \"\"\", {\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'raw_code': chunk,\n",
    "                        'summary': chunk_summary,\n",
    "                        'summary_embedding': summary_embedding,\n",
    "                        'chunk_embedding': chunk_embedding,\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                    # Extract and process code entities for each chunk\n",
    "                    chunk_entities = extract_code_entities(chunk)\n",
    "\n",
    "                    # Collect unique imports\n",
    "                    for imp in chunk_entities['imports']:\n",
    "                        module_name = imp['module']\n",
    "                        if module_name not in unique_imports:\n",
    "                            unique_imports[module_name] = set(imp['entities'])\n",
    "                        else:\n",
    "                            unique_imports[module_name].update(imp['entities'])\n",
    "\n",
    "                    # Collect unique functions\n",
    "                    for func in chunk_entities['functions']:\n",
    "                        func_name = func['name']\n",
    "                        if func_name not in unique_functions:\n",
    "                            unique_functions[func_name] = {\n",
    "                                'parameters': json.dumps([{'name': param['name'], 'type': param['type']} for param in func['parameters']]),\n",
    "                                'return_type': func['return_type']\n",
    "                            }\n",
    "\n",
    "                    # Collect unique classes\n",
    "                    for cls in chunk_entities['classes']:\n",
    "                        cls_name = cls['name']\n",
    "                        if cls_name not in unique_classes:\n",
    "                            unique_classes[cls_name] = {\n",
    "                                'parameters': json.dumps([{'name': param['name'], 'type': param['type']} for param in cls['parameters']])\n",
    "                            }\n",
    "\n",
    "                # Store unique entities in the graph\n",
    "\n",
    "                # Create and link unique import nodes\n",
    "                for module, entities in unique_imports.items():\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (import:Import {module: $module})\n",
    "                        SET import.entities = $entities\n",
    "                        WITH import\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:CALLS]->(import)\n",
    "                    \"\"\", {\n",
    "                        'module': module,\n",
    "                        'entities': list(entities),\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                # Create and link unique function nodes\n",
    "                for name, func in unique_functions.items():\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (function:Function {name: $name})\n",
    "                        SET function.parameters = $parameters, function.return_type = $return_type\n",
    "                        WITH function\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:DEFINES]->(function)\n",
    "                    \"\"\", {\n",
    "                        'name': name,\n",
    "                        'parameters': func['parameters'],\n",
    "                        'return_type': func['return_type'],\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                # Create and link unique class nodes\n",
    "                for name, cls in unique_classes.items():\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (class:Class {name: $name})\n",
    "                        SET class.parameters = $parameters\n",
    "                        WITH class\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:DEFINES]->(class)\n",
    "                    \"\"\", {\n",
    "                        'name': name,\n",
    "                        'parameters': cls['parameters'],\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                # Link all chunks in sequence using APOC's `NEXT` relationship\n",
    "                self.kg.query(\"\"\"\n",
    "                    MATCH (file:File {path: $file_path})-[:CONTAINS]->(chunk:Chunk)\n",
    "                    WITH chunk ORDER BY chunk.id ASC\n",
    "                    WITH collect(chunk) AS chunks\n",
    "                    CALL apoc.nodes.link(chunks, 'NEXT')\n",
    "                    RETURN count(*)\n",
    "                \"\"\", {\n",
    "                    'file_path': file\n",
    "                })\n",
    "\n",
    "    def _find_files_without_summaries(self):\n",
    "        \"\"\"\n",
    "        Find all files in the graph that do not have summaries.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of file paths that do not have summaries.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (file:File)\n",
    "        WHERE file.summary IS NULL\n",
    "        RETURN file.path AS file_path\n",
    "        \"\"\"\n",
    "        result = self.kg.query(query)\n",
    "        return [record['file_path'] for record in result]\n",
    "\n",
    "    def _find_directories_without_summaries(self):\n",
    "        \"\"\"\n",
    "        Find all directories in the graph that do not have summaries.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of directory paths that do not have summaries.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (dir:Directory)\n",
    "        WHERE dir.summary IS NULL\n",
    "        RETURN dir.path AS dir_path\n",
    "        \"\"\"\n",
    "        result = self.kg.query(query)\n",
    "        return [record['dir_path'] for record in result]\n",
    "    \n",
    "    def _find_nodes_without_embeddings(self):\n",
    "        \"\"\"\n",
    "        Find all files and directories in the graph that have summaries but do not have embeddings.\n",
    "\n",
    "        Returns:\n",
    "            List[dict]: A list of dictionaries containing the node type ('File' or 'Directory') and the path.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE n.summary IS NOT NULL AND n.summary_embedding IS NULL AND (n:File OR n:Directory)\n",
    "        RETURN labels(n) AS node_type, n.path AS node_path\n",
    "        \"\"\"\n",
    "        result = self.kg.query(query)\n",
    "        return [{'node_type': record['node_type'][0], 'node_path': record['node_path']} for record in result]\n",
    "    \n",
    "    \n",
    "    def _summarize_file_from_chunks(self, file_path):\n",
    "        \"\"\"\n",
    "        Summarize a file based on the summaries of its chunks.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file to summarize.\n",
    "\n",
    "        Returns:\n",
    "            str: The summary of the file.\n",
    "        \"\"\"\n",
    "        # Query to get summaries of all chunks associated with the file\n",
    "        query = \"\"\"\n",
    "        MATCH (file:File {path: $file_path})-[:CONTAINS]->(chunk:Chunk)\n",
    "        RETURN chunk.summary AS chunk_summary\n",
    "        ORDER BY chunk.id ASC\n",
    "        \"\"\"\n",
    "        result = self.kg.query(query, {'file_path': file_path})\n",
    "        chunk_summaries = ['Chunk summaries'] + [record['chunk_summary'] for record in result]\n",
    "\n",
    "        # Summarize the list of chunk summaries\n",
    "        file_summary = summarize_list_of_summaries(chunk_summaries)\n",
    "\n",
    "        # Store the file summary in the graph under the \"summary\" attribute\n",
    "        self.kg.query(\"\"\"\n",
    "            MATCH (file:File {path: $file_path})\n",
    "            SET file.summary = $file_summary\n",
    "        \"\"\", {\n",
    "            'file_path': file_path,\n",
    "            'file_summary': file_summary\n",
    "        })\n",
    "\n",
    "        return file_summary\n",
    "    \n",
    "    def _summarize_directory(self, directory_path):\n",
    "        \"\"\"\n",
    "        Summarize a directory based on the summaries of its files and subdirectories.\n",
    "\n",
    "        Args:\n",
    "            directory_path (str): The path to the directory to summarize.\n",
    "\n",
    "        Returns:\n",
    "            str: The summary of the directory.\n",
    "        \"\"\"\n",
    "        # Query to get summaries of all files directly contained in the directory\n",
    "        file_query = \"\"\"\n",
    "        MATCH (dir:Directory {path: $directory_path})-[:CONTAINS]->(file:File)\n",
    "        RETURN file.summary AS file_summary\n",
    "        \"\"\"\n",
    "        file_result = self.kg.query(file_query, {'directory_path': directory_path})\n",
    "        file_summaries = [record['file_summary'] for record in file_result]\n",
    "\n",
    "        # Query to get all subdirectories directly contained in the directory\n",
    "        subdir_query = \"\"\"\n",
    "        MATCH (dir:Directory {path: $directory_path})-[:CONTAINS]->(subdir:Directory)\n",
    "        RETURN subdir.path AS subdir_path\n",
    "        \"\"\"\n",
    "        subdir_result = self.kg.query(subdir_query, {'directory_path': directory_path})\n",
    "        subdir_paths = [record['subdir_path'] for record in subdir_result]\n",
    "\n",
    "        # Recursively summarize each subdirectory if it doesn't already have a summary\n",
    "        subdir_summaries = []\n",
    "        for subdir_path in subdir_paths:\n",
    "            subdir_summary = self._summarize_directory(subdir_path)\n",
    "            subdir_summaries.append(subdir_summary)\n",
    "\n",
    "        # Combine file summaries and subdirectory summaries\n",
    "        all_summaries = ['File summaries: '] + file_summaries + ['Subdirectory summaries: ']  + subdir_summaries\n",
    "\n",
    "        # Summarize the list of all summaries (files + subdirectories)\n",
    "        directory_summary = summarize_list_of_summaries(all_summaries)\n",
    "\n",
    "        # Store the directory summary in the graph under the \"summary\" attribute\n",
    "        self.kg.query(\"\"\"\n",
    "            MATCH (dir:Directory {path: $directory_path})\n",
    "            SET dir.summary = $directory_summary\n",
    "        \"\"\", {\n",
    "            'directory_path': directory_path,\n",
    "            'directory_summary': directory_summary\n",
    "        })\n",
    "\n",
    "        return directory_summary\n",
    "\n",
    "    def _automate_summarization(self):\n",
    "        \"\"\"\n",
    "        Automate the summarization process for files and directories in the graph.\n",
    "        \"\"\"\n",
    "        # Summarize files without summaries\n",
    "        files_without_summaries = self._find_files_without_summaries()\n",
    "        for file_path in tqdm(files_without_summaries, desc='Summarizing files'):\n",
    "            self._summarize_file_from_chunks(file_path)\n",
    "\n",
    "        # Summarize directories without summaries\n",
    "        directories_without_summaries = self._find_directories_without_summaries()\n",
    "        for dir_path in tqdm(directories_without_summaries, 'Summarizing directories'):\n",
    "            self._summarize_directory(dir_path)\n",
    "\n",
    "        # Generate embeddings for nodes that have summaries but no embeddings\n",
    "        self._generate_and_store_embeddings()\n",
    "\n",
    "    def _generate_and_store_embeddings(self):\n",
    "        \"\"\"\n",
    "        Generate embeddings for files and directories that have summaries but lack embeddings.\n",
    "        \"\"\"\n",
    "        nodes_without_embeddings = self._find_nodes_without_embeddings()\n",
    "\n",
    "        for node in tqdm(nodes_without_embeddings, desc='Creating File and Directory embeddings'):\n",
    "            # Retrieve the summary of the node\n",
    "            query = f\"\"\"\n",
    "            MATCH (n:{node['node_type']} {{path: $node_path}})\n",
    "            RETURN n.summary AS summary\n",
    "            \"\"\"\n",
    "            result = self.kg.query(query, {'node_path': node['node_path']})\n",
    "            summary = result[0]['summary']\n",
    "\n",
    "            # Generate the embedding for the summary\n",
    "            embedding = get_embedding(summary)\n",
    "\n",
    "            # Store the embedding back in the graph\n",
    "            query = f\"\"\"\n",
    "            MATCH (n:{node['node_type']} {{path: $node_path}})\n",
    "            SET n.summary_embedding = $embedding\n",
    "            \"\"\"\n",
    "            self.kg.query(query, {\n",
    "                'node_path': node['node_path'],\n",
    "                'embedding': embedding\n",
    "            })\n",
    "\n",
    "    def _create_vector_index(self, label, property_name=\"summary_embeddings\", index_name=None, dimensions=1536):\n",
    "        \"\"\"\n",
    "        Create a vector index for the specified label if it does not already exist.\n",
    "\n",
    "        Args:\n",
    "            label (str): The label of the nodes (e.g., 'File', 'Directory', 'Chunk').\n",
    "            property_name (str): The property name on which the vector index is created. Default is 'summary_embeddings'.\n",
    "            index_name (str): The name of the index. If None, it will default to 'labelVectorIndex'.\n",
    "            dimensions (int): The dimensionality of the vectors. Default is 1536.\n",
    "        \"\"\"\n",
    "        if not index_name:\n",
    "            index_name = f\"{label.lower()}VectorIndex\"\n",
    "\n",
    "        query = f\"\"\"\n",
    "        CREATE VECTOR INDEX {index_name} IF NOT EXISTS\n",
    "        FOR (n:{label})\n",
    "        ON n.{property_name}\n",
    "        OPTIONS {{\n",
    "            indexConfig: {{\n",
    "                `vector.dimensions`: {dimensions},\n",
    "                `vector.similarity_function`: 'cosine'\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.kg.query(query)\n",
    "            print(f\"Vector index {index_name} for label {label} created successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while creating the vector index: {e}\")\n",
    "\n",
    "    def _create_all_vector_indexes(self):\n",
    "        \"\"\"\n",
    "        Create vector indexes for chunks, files, and directories. The indexes are separated for chunks and summaries.\n",
    "        \"\"\"\n",
    "        # Create index for chunks\n",
    "        self._create_vector_index(label=\"Chunk\", property_name=\"chunk_embeddings\", index_name=\"chunkRawVectorIndex\")\n",
    "        self._create_vector_index(label=\"Chunk\", property_name=\"summary_embeddings\", index_name=\"chunkSummaryVectorIndex\")\n",
    "\n",
    "        # Create index for files and directories\n",
    "        self._create_vector_index(label=\"File\", property_name=\"summary_embeddings\", index_name=\"fileSummaryVectorIndex\")\n",
    "        self._create_vector_index(label=\"Directory\", property_name=\"summary_embeddings\", index_name=\"dirSummaryVectorIndex\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def create_graph(self):\n",
    "\n",
    "        self._load_dirs_and_files_to_graph()\n",
    "\n",
    "        self._enrich_graph()\n",
    "\n",
    "        self._automate_summarization()\n",
    "\n",
    "        self._generate_and_store_embeddings()\n",
    "\n",
    "        self._create_all_vector_indexes()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refactor since class has gotten huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_contents(file_path):\n",
    "    \"\"\"\n",
    "    Opens a file and reads its contents as text.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to be read.\n",
    "\n",
    "    Returns:\n",
    "        str: The contents of the file as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            contents = file.read()\n",
    "        return contents\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        return None\n",
    "\n",
    "def should_skip_file(file_path):\n",
    "    \"\"\"\n",
    "    Determine if a file should be skipped based on its type or size.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file should be skipped, False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the file extension\n",
    "    _, ext = os.path.splitext(file_path)\n",
    "\n",
    "    # Define file types to skip\n",
    "    skip_extensions = {\n",
    "        '.lock', '.png', '.jpg', '.gif', '.pdf', '.zip', '.class', '.o', '.out',\n",
    "        '.md', '.rst', '.csv', '.tsv'\n",
    "    }\n",
    "\n",
    "    # Define directories to skip\n",
    "    skip_directories = {'node_modules', '.git', '.svn'}\n",
    "\n",
    "    # Check if the file extension is in the skip list\n",
    "    if ext.lower() in skip_extensions:\n",
    "        return True\n",
    "\n",
    "    # Check if the file is in a directory that should be skipped\n",
    "    if any(skip_dir in file_path for skip_dir in skip_directories):\n",
    "        return True\n",
    "\n",
    "    # Optionally, skip large files (e.g., > 5MB)\n",
    "    if os.path.getsize(file_path) > 5 * 1024 * 1024:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "class FileSystemProcessor:\n",
    "    def __init__(\n",
    "            self, \n",
    "            root_directory \n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the CodebaseGraph with a connection to Neo4j.\n",
    "\n",
    "        Args:\n",
    "            root_directory (str): The directory to be extracted into knowledge.\n",
    "        \"\"\"\n",
    "\n",
    "        self.root_directory = root_directory\n",
    "        \n",
    "\n",
    "    def _get_file_info(self, file_path):\n",
    "        \"\"\"\n",
    "        Get information about a file, including type, size, last modified date, creation date, permissions, owner, and hash.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing file information.\n",
    "        \"\"\"\n",
    "        stats = os.stat(file_path)\n",
    "        file_type = os.path.splitext(file_path)[1][1:]  # Get file extension without the dot\n",
    "        size = stats.st_size\n",
    "        last_modified = datetime.fromtimestamp(stats.st_mtime).isoformat()\n",
    "        created = datetime.fromtimestamp(stats.st_ctime).isoformat()\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"type\": file_type,\n",
    "            \"size\": size,\n",
    "            \"last_modified\": last_modified,\n",
    "            \"created\": created,\n",
    "        }\n",
    "\n",
    "    def load_dirs_and_files_to_graph(self, kg):\n",
    "        \"\"\"\n",
    "        Traverse a directory and create a graph in Neo4j representing the directory structure and file information.\n",
    "\n",
    "        Args:\n",
    "            kg (Neo4jGraph): graph object to complete cypher queries\n",
    "        \"\"\"\n",
    "        print(\"Creating file and dir nodes from Walk\")\n",
    "        for root, dirs, files in os.walk(self.root_directory):\n",
    "\n",
    "            dir_name = os.path.basename(root)\n",
    "\n",
    "            # Create node for the directory\n",
    "            kg.query(\n",
    "                \"\"\"\n",
    "                MERGE (dir:Directory {name: $dir_name, path: $path})\n",
    "                ON CREATE SET dir.created = $created, dir.last_modified = $last_modified\n",
    "                \"\"\",\n",
    "                {\n",
    "                    'dir_name':dir_name,\n",
    "                    'path':root,\n",
    "                    'created':datetime.fromtimestamp(os.stat(root).st_ctime).isoformat(),\n",
    "                    'last_modified':datetime.fromtimestamp(os.stat(root).st_mtime).isoformat(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Create nodes for subdirectories\n",
    "            for dir_name in dirs:\n",
    "                dir_path = os.path.join(root, dir_name)\n",
    "                kg.query(\n",
    "                    \"\"\"\n",
    "                        MERGE (subdir:Directory {name: $dir_name, path: $subdir_path})\n",
    "                        ON CREATE SET subdir.created = $created, subdir.last_modified = $last_modified\n",
    "                        WITH subdir\n",
    "                        MATCH (parent:Directory {path: $parent_path})\n",
    "                        MERGE (parent)-[:CONTAINS]->(subdir)\n",
    "                    \"\"\",\n",
    "                    {\n",
    "                        'dir_name':dir_name,\n",
    "                        'subdir_path':dir_path, \n",
    "                        'parent_path':root,\n",
    "                        'created':datetime.fromtimestamp(os.stat(dir_path).st_ctime).isoformat(),\n",
    "                        'last_modified':datetime.fromtimestamp(os.stat(dir_path).st_mtime).isoformat(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Create nodes for files\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                file_info = self._get_file_info(file_path)\n",
    "                kg.query(\n",
    "                    \"\"\"\n",
    "                        MERGE (file:File {name: $file_name, path: $file_path})\n",
    "                        ON CREATE SET file.type = $type, file.size = $size, file.last_modified = $last_modified, file.created = $created\n",
    "                        WITH file\n",
    "                        MATCH (parent:Directory {path: $parent_path})\n",
    "                        MERGE (parent)-[:CONTAINS]->(file)\n",
    "                    \"\"\",\n",
    "                    {\n",
    "                        'file_name':file_name, \n",
    "                        'file_path':file_path, \n",
    "                        'type':file_info['type'], \n",
    "                        'size':file_info['size'], \n",
    "                        'last_modified':file_info['last_modified'], \n",
    "                        'created':file_info['created'], \n",
    "                        'parent_path':root\n",
    "                    }\n",
    "                )\n",
    "\n",
    "class GraphBuilder:\n",
    "    def __init__(\n",
    "            self, \n",
    "            kg,\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=25\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the CodebaseGraph with a connection to Neo4j.\n",
    "\n",
    "        Args:\n",
    "            kg (Neo4jGraph): graph object to complete cypher queries\n",
    "            chunk_size (int): size of chunk to use (by number of tokens)\n",
    "            chunk_overlap (int): number of chunks to overlap when splitting\n",
    "        \"\"\"\n",
    "        self.kg = kg\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def enrich_graph(self):\n",
    "        \"\"\"\n",
    "        Enriches the knowledge graph by processing files, creating and linking code chunks, and extracting unique code entities.\n",
    "        \"\"\"\n",
    "        text_splitter = PythonCodeTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "\n",
    "        # Query to find files without chunk nodes\n",
    "        query = \"\"\"\n",
    "        MATCH (file:File)\n",
    "        WHERE NOT (file)-[:CONTAINS]->(:Chunk)\n",
    "        RETURN file.path AS file_path\n",
    "        \"\"\"\n",
    "\n",
    "        result = self.kg.query(query)\n",
    "        file_paths = [record['file_path'] for record in result]\n",
    "\n",
    "        file_paths = [file for file in file_paths if not should_skip_file(file)]\n",
    "\n",
    "        for file in tqdm(file_paths, desc='Creating chunks from files'):\n",
    "            file_contents = read_file_contents(file)\n",
    "\n",
    "            if file_contents is not None:\n",
    "                chunks = text_splitter.split_text(file_contents)\n",
    "\n",
    "                # Initialize collections for unique entities\n",
    "                unique_imports = {}\n",
    "                unique_functions = {}\n",
    "                unique_classes = {}\n",
    "\n",
    "                for idx, chunk in enumerate(chunks):\n",
    "                    chunk_id = f\"{file}_chunk_{idx:06d}\"\n",
    "                    chunk_summary = summarize_file_chunk(chunk_text=chunk, file_name=file)\n",
    "                    summary_embedding = get_embedding(chunk_summary)\n",
    "                    chunk_embedding = get_embedding(chunk)\n",
    "\n",
    "                    # Create the chunk node and link it to the file\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (chunk:Chunk {id: $chunk_id})\n",
    "                        SET chunk.raw_code = $raw_code, \n",
    "                            chunk.summary = $summary, \n",
    "                            chunk.summary_embedding = $summary_embedding, \n",
    "                            chunk.chunk_embedding = $chunk_embedding\n",
    "                        WITH chunk\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:CONTAINS]->(chunk)\n",
    "                    \"\"\", {\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'raw_code': chunk,\n",
    "                        'summary': chunk_summary,\n",
    "                        'summary_embedding': summary_embedding,\n",
    "                        'chunk_embedding': chunk_embedding,\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                    # Extract and process code entities for each chunk\n",
    "                    chunk_entities = extract_code_entities(chunk)\n",
    "\n",
    "                    # Collect unique imports\n",
    "                    for imp in chunk_entities['imports']:\n",
    "                        module_name = imp['module']\n",
    "                        if module_name not in unique_imports:\n",
    "                            unique_imports[module_name] = set(imp['entities'])\n",
    "                        else:\n",
    "                            unique_imports[module_name].update(imp['entities'])\n",
    "\n",
    "                    # Collect unique functions\n",
    "                    for func in chunk_entities['functions']:\n",
    "                        func_name = func['name']\n",
    "                        if func_name not in unique_functions:\n",
    "                            unique_functions[func_name] = {\n",
    "                                'parameters': json.dumps([{'name': param['name'], 'type': param['type']} for param in func['parameters']]),\n",
    "                                'return_type': func['return_type']\n",
    "                            }\n",
    "\n",
    "                    # Collect unique classes\n",
    "                    for cls in chunk_entities['classes']:\n",
    "                        cls_name = cls['name']\n",
    "                        if cls_name not in unique_classes:\n",
    "                            unique_classes[cls_name] = {\n",
    "                                'parameters': json.dumps([{'name': param['name'], 'type': param['type']} for param in cls['parameters']])\n",
    "                            }\n",
    "\n",
    "                # Store unique entities in the graph\n",
    "\n",
    "                # Create and link unique import nodes\n",
    "                for module, entities in unique_imports.items():\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (import:Import {module: $module})\n",
    "                        SET import.entities = $entities\n",
    "                        WITH import\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:CALLS]->(import)\n",
    "                    \"\"\", {\n",
    "                        'module': module,\n",
    "                        'entities': list(entities),\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                # Create and link unique function nodes\n",
    "                for name, func in unique_functions.items():\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (function:Function {name: $name})\n",
    "                        SET function.parameters = $parameters, function.return_type = $return_type\n",
    "                        WITH function\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:DEFINES]->(function)\n",
    "                    \"\"\", {\n",
    "                        'name': name,\n",
    "                        'parameters': func['parameters'],\n",
    "                        'return_type': func['return_type'],\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                # Create and link unique class nodes\n",
    "                for name, cls in unique_classes.items():\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (class:Class {name: $name})\n",
    "                        SET class.parameters = $parameters\n",
    "                        WITH class\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:DEFINES]->(class)\n",
    "                    \"\"\", {\n",
    "                        'name': name,\n",
    "                        'parameters': cls['parameters'],\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                # Link all chunks in sequence using APOC's `NEXT` relationship\n",
    "                self.kg.query(\"\"\"\n",
    "                    MATCH (file:File {path: $file_path})-[:CONTAINS]->(chunk:Chunk)\n",
    "                    WITH chunk ORDER BY chunk.id ASC\n",
    "                    WITH collect(chunk) AS chunks\n",
    "                    CALL apoc.nodes.link(chunks, 'NEXT')\n",
    "                    RETURN count(*)\n",
    "                \"\"\", {\n",
    "                    'file_path': file\n",
    "                })\n",
    "\n",
    "    def _create_vector_index(self, label, property_name=\"summary_embeddings\", index_name=None, dimensions=1536):\n",
    "        \"\"\"\n",
    "        Create a vector index for the specified label if it does not already exist.\n",
    "\n",
    "        Args:\n",
    "            label (str): The label of the nodes (e.g., 'File', 'Directory', 'Chunk').\n",
    "            property_name (str): The property name on which the vector index is created. Default is 'summary_embeddings'.\n",
    "            index_name (str): The name of the index. If None, it will default to 'labelVectorIndex'.\n",
    "            dimensions (int): The dimensionality of the vectors. Default is 1536.\n",
    "        \"\"\"\n",
    "        if not index_name:\n",
    "            index_name = f\"{label.lower()}VectorIndex\"\n",
    "\n",
    "        query = f\"\"\"\n",
    "        CREATE VECTOR INDEX {index_name} IF NOT EXISTS\n",
    "        FOR (n:{label})\n",
    "        ON n.{property_name}\n",
    "        OPTIONS {{\n",
    "            indexConfig: {{\n",
    "                `vector.dimensions`: {dimensions},\n",
    "                `vector.similarity_function`: 'cosine'\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.kg.query(query)\n",
    "            print(f\"Vector index {index_name} for label {label} created successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while creating the vector index: {e}\")\n",
    "\n",
    "    def create_all_vector_indexes(self):\n",
    "        \"\"\"\n",
    "        Create vector indexes for chunks, files, and directories. The indexes are separated for chunks and summaries.\n",
    "        \"\"\"\n",
    "        # Create index for chunks\n",
    "        self._create_vector_index(label=\"Chunk\", property_name=\"chunk_embeddings\", index_name=\"chunkRawVectorIndex\")\n",
    "        self._create_vector_index(label=\"Chunk\", property_name=\"summary_embeddings\", index_name=\"chunkSummaryVectorIndex\")\n",
    "\n",
    "        # Create index for files and directories\n",
    "        self._create_vector_index(label=\"File\", property_name=\"summary_embeddings\", index_name=\"fileSummaryVectorIndex\")\n",
    "        self._create_vector_index(label=\"Directory\", property_name=\"summary_embeddings\", index_name=\"dirSummaryVectorIndex\")\n",
    "\n",
    "class SummaryManager:\n",
    "    def __init__(\n",
    "            self, \n",
    "            kg\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the CodebaseGraph with a connection to Neo4j.\n",
    "\n",
    "        Args:\n",
    "            kg (Neo4jGraph): graph object to complete cypher queries\n",
    "        \"\"\"\n",
    "        self.kg = kg\n",
    "    \n",
    "    def _find_files_without_summaries(self):\n",
    "        \"\"\"\n",
    "        Find all files in the graph that do not have summaries.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of file paths that do not have summaries.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (file:File)\n",
    "        WHERE file.summary IS NULL\n",
    "        RETURN file.path AS file_path\n",
    "        \"\"\"\n",
    "        result = self.kg.query(query)\n",
    "        return [record['file_path'] for record in result]\n",
    "\n",
    "    def _find_directories_without_summaries(self):\n",
    "        \"\"\"\n",
    "        Find all directories in the graph that do not have summaries.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of directory paths that do not have summaries.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (dir:Directory)\n",
    "        WHERE dir.summary IS NULL\n",
    "        RETURN dir.path AS dir_path\n",
    "        \"\"\"\n",
    "        result = self.kg.query(query)\n",
    "        return [record['dir_path'] for record in result]\n",
    "    \n",
    "    def _find_nodes_without_embeddings(self):\n",
    "        \"\"\"\n",
    "        Find all files and directories in the graph that have summaries but do not have embeddings.\n",
    "\n",
    "        Returns:\n",
    "            List[dict]: A list of dictionaries containing the node type ('File' or 'Directory') and the path.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE n.summary IS NOT NULL AND n.summary_embedding IS NULL AND (n:File OR n:Directory)\n",
    "        RETURN labels(n) AS node_type, n.path AS node_path\n",
    "        \"\"\"\n",
    "        result = self.kg.query(query)\n",
    "        return [{'node_type': record['node_type'][0], 'node_path': record['node_path']} for record in result]\n",
    "    \n",
    "    \n",
    "    def _summarize_file_from_chunks(self, file_path):\n",
    "        \"\"\"\n",
    "        Summarize a file based on the summaries of its chunks.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file to summarize.\n",
    "\n",
    "        Returns:\n",
    "            str: The summary of the file.\n",
    "        \"\"\"\n",
    "        # Query to get summaries of all chunks associated with the file\n",
    "        query = \"\"\"\n",
    "        MATCH (file:File {path: $file_path})-[:CONTAINS]->(chunk:Chunk)\n",
    "        RETURN chunk.summary AS chunk_summary\n",
    "        ORDER BY chunk.id ASC\n",
    "        \"\"\"\n",
    "        result = self.kg.query(query, {'file_path': file_path})\n",
    "        chunk_summaries = ['Chunk summaries'] + [record['chunk_summary'] for record in result]\n",
    "\n",
    "        # Summarize the list of chunk summaries\n",
    "        file_summary = summarize_list_of_summaries(chunk_summaries)\n",
    "\n",
    "        # Store the file summary in the graph under the \"summary\" attribute\n",
    "        self.kg.query(\"\"\"\n",
    "            MATCH (file:File {path: $file_path})\n",
    "            SET file.summary = $file_summary\n",
    "        \"\"\", {\n",
    "            'file_path': file_path,\n",
    "            'file_summary': file_summary\n",
    "        })\n",
    "\n",
    "        return file_summary\n",
    "    \n",
    "    def _summarize_directory(self, directory_path):\n",
    "        \"\"\"\n",
    "        Summarize a directory based on the summaries of its files and subdirectories.\n",
    "\n",
    "        Args:\n",
    "            directory_path (str): The path to the directory to summarize.\n",
    "\n",
    "        Returns:\n",
    "            str: The summary of the directory.\n",
    "        \"\"\"\n",
    "        # Query to get summaries of all files directly contained in the directory\n",
    "        file_query = \"\"\"\n",
    "        MATCH (dir:Directory {path: $directory_path})-[:CONTAINS]->(file:File)\n",
    "        RETURN file.summary AS file_summary\n",
    "        \"\"\"\n",
    "        file_result = self.kg.query(file_query, {'directory_path': directory_path})\n",
    "        file_summaries = [record['file_summary'] for record in file_result]\n",
    "\n",
    "        # Query to get all subdirectories directly contained in the directory\n",
    "        subdir_query = \"\"\"\n",
    "        MATCH (dir:Directory {path: $directory_path})-[:CONTAINS]->(subdir:Directory)\n",
    "        RETURN subdir.path AS subdir_path\n",
    "        \"\"\"\n",
    "        subdir_result = self.kg.query(subdir_query, {'directory_path': directory_path})\n",
    "        subdir_paths = [record['subdir_path'] for record in subdir_result]\n",
    "\n",
    "        # Recursively summarize each subdirectory if it doesn't already have a summary\n",
    "        subdir_summaries = []\n",
    "        for subdir_path in subdir_paths:\n",
    "            subdir_summary = self._summarize_directory(subdir_path)\n",
    "            subdir_summaries.append(subdir_summary)\n",
    "\n",
    "        # Combine file summaries and subdirectory summaries\n",
    "        all_summaries = ['File summaries: '] + file_summaries + ['Subdirectory summaries: ']  + subdir_summaries\n",
    "\n",
    "        # Summarize the list of all summaries (files + subdirectories)\n",
    "        directory_summary = summarize_list_of_summaries(all_summaries)\n",
    "\n",
    "        # Store the directory summary in the graph under the \"summary\" attribute\n",
    "        self.kg.query(\"\"\"\n",
    "            MATCH (dir:Directory {path: $directory_path})\n",
    "            SET dir.summary = $directory_summary\n",
    "        \"\"\", {\n",
    "            'directory_path': directory_path,\n",
    "            'directory_summary': directory_summary\n",
    "        })\n",
    "\n",
    "        return directory_summary\n",
    "\n",
    "    def _generate_and_store_embeddings(self):\n",
    "        \"\"\"\n",
    "        Generate embeddings for files and directories that have summaries but lack embeddings.\n",
    "        \"\"\"\n",
    "        nodes_without_embeddings = self._find_nodes_without_embeddings()\n",
    "\n",
    "        for node in tqdm(nodes_without_embeddings, desc='Creating File and Directory embeddings'):\n",
    "            # Retrieve the summary of the node\n",
    "            query = f\"\"\"\n",
    "            MATCH (n:{node['node_type']} {{path: $node_path}})\n",
    "            RETURN n.summary AS summary\n",
    "            \"\"\"\n",
    "            result = self.kg.query(query, {'node_path': node['node_path']})\n",
    "            summary = result[0]['summary']\n",
    "\n",
    "            # Generate the embedding for the summary\n",
    "            embedding = get_embedding(summary)\n",
    "\n",
    "            # Store the embedding back in the graph\n",
    "            query = f\"\"\"\n",
    "            MATCH (n:{node['node_type']} {{path: $node_path}})\n",
    "            SET n.summary_embedding = $embedding\n",
    "            \"\"\"\n",
    "            self.kg.query(query, {\n",
    "                'node_path': node['node_path'],\n",
    "                'embedding': embedding\n",
    "            })\n",
    "\n",
    "    def automate_summarization(self):\n",
    "        \"\"\"\n",
    "        Automate the summarization process for files and directories in the graph.\n",
    "        \"\"\"\n",
    "        # Summarize files without summaries\n",
    "        files_without_summaries = self._find_files_without_summaries()\n",
    "        for file_path in tqdm(files_without_summaries, desc='Summarizing files'):\n",
    "            self._summarize_file_from_chunks(file_path)\n",
    "\n",
    "        # Summarize directories without summaries\n",
    "        directories_without_summaries = self._find_directories_without_summaries()\n",
    "        for dir_path in tqdm(directories_without_summaries, 'Summarizing directories'):\n",
    "            self._summarize_directory(dir_path)\n",
    "\n",
    "        # Generate embeddings for nodes that have summaries but no embeddings\n",
    "        self._generate_and_store_embeddings()\n",
    "\n",
    "class CodebaseGraph:\n",
    "    def __init__(\n",
    "            self, \n",
    "            root_directory, \n",
    "            uri=\"bolt://localhost:7687\", \n",
    "            user=None, \n",
    "            password=None, \n",
    "            openai_api_key=None,\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=25\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the CodebaseGraph with a connection to Neo4j.\n",
    "\n",
    "        Args:\n",
    "            root_directory (str): The directory to be extracted into knowledge.\n",
    "            uri (str): The URI of the Neo4j database. Defaults to localhost.\n",
    "            user (str): Username for Neo4j. If None, loads from environment variable NEO4J_USERNAME.\n",
    "            password (str): Password for Neo4j. If None, loads from environment variable NEO4J_PASSWORD.\n",
    "            openai_api_key (str): Key needed to access OpenAI API\n",
    "            chunk_size (int): size of chunk to use (by number of tokens)\n",
    "            chunk_overlap (int): number of chunks to overlap when splitting\n",
    "        \"\"\"\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "\n",
    "        # Set up the Neo4j connection\n",
    "        self.uri = uri\n",
    "        self.NEO4J_USER = user or os.getenv(\"NEO4J_USERNAME\")\n",
    "        self.NEO4J_PASSWORD =  password or os.getenv(\"NEO4J_PASSWORD\")\n",
    "        self.OPENAI_API_KEY = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        if not self.NEO4J_USER or not self.NEO4J_PASSWORD:\n",
    "            raise ValueError(\"NEO4J_USERNAME and NEO4J_PASSWORD must be provided either as arguments or environment variables.\")\n",
    "        \n",
    "        if not self.OPENAI_API_KEY:\n",
    "            raise ValueError(\"NEO4J_USERNAME and NEO4J_PASSWORD must be provided either as arguments or environment variables.\")\n",
    "\n",
    "        self.kg = connect_to_neo4j()\n",
    "\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "        self.fs_processor = FileSystemProcessor(root_directory)\n",
    "        self.graph_builder = GraphBuilder(\n",
    "            self.kg, \n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap\n",
    "        )\n",
    "        self.summary_manager = SummaryManager(self.kg)\n",
    "\n",
    "    def create_graph(self):\n",
    "        self.fs_processor.load_dirs_and_files_to_graph(self.kg)\n",
    "        self.graph_builder.enrich_graph()\n",
    "        self.summary_manager.automate_summarization()\n",
    "        self.graph_builder.create_all_vector_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "codebase_graph = CodebaseGraph(\n",
    "    root_directory=\"C:\\\\Users\\\\willd\\\\Documents\\\\Git\\\\graphRag\\\\test_project\"\n",
    ")\n",
    "codebase_graph.create_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to improve summarizations. Do this in two steps:\n",
    "\n",
    "1. Create an ascii visual of the file system to be passed to the summarization components (for files and dirs)\n",
    "2. Create a markdown template for the summaries, possibly splitting the file and dir components\n",
    "\n",
    "Step (2) can be modified as is from the `build_tools\\utils.py` and `summary_tools\\utils.py` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from edoc.gpt_helpers.gpt_basics import create_chat_completion\n",
    "\n",
    "def generate_ascii_structure(root_directory, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Generates an ASCII file structure from the root directory using OpenAI's language model.\n",
    "\n",
    "    Args:\n",
    "        root_directory (str): The root directory to summarize.\n",
    "        model (str): The OpenAI model to use. Default is 'gpt-4o-mini'.\n",
    "\n",
    "    Returns:\n",
    "        str: The ASCII file structure summarized by the model.\n",
    "    \"\"\"\n",
    "    # Step 1: Create a basic tree structure using os.walk()\n",
    "    root_directory = str(root_directory)\n",
    "    \n",
    "    file_structure = \"\"\n",
    "    for root, dirs, files in os.walk(root_directory):\n",
    "        # Calculate the level of depth (indentation)\n",
    "        root = str(root)\n",
    "        level = root.replace(root_directory, '').count(os.sep)\n",
    "        indent = ' ' * 4 * level\n",
    "        # Add directory\n",
    "        file_structure += f\"{indent}{os.path.basename(root)}/\\n\"\n",
    "        # Add files within the directory\n",
    "        sub_indent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            file_structure += f\"{sub_indent}{f}\\n\"\n",
    "\n",
    "    # Step 2: Generate the prompt for the LLM\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"I have the following file structure:\\n{file_structure}\\nPlease convert this into a clean and simple ASCII tree format. No need for any extra words, just the tree please.\"}\n",
    "    ]\n",
    "    \n",
    "    # Step 3: Use the LLM to refine the file structure into a well-formatted ASCII tree\n",
    "    ascii_tree = create_chat_completion(messages=prompt, model=model)\n",
    "    \n",
    "    return ascii_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_structure_as_str = generate_ascii_structure(root_directory=\"C:\\\\Users\\\\willd\\\\Documents\\\\Git\\\\resume_website\\\\react-resume-template\")\n",
    "print(ascii_structure_as_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edoc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
