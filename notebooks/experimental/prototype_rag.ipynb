{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from edoc.gpt_helpers.gpt_basics import create_chat_completion\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First step is to set up retrievers for the unstructured lookup. This will make use of the vector index.\n",
    "\n",
    "- create extraction for named code entities in the text\n",
    "- create vector lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgrammingNamedEntities(BaseModel):\n",
    "    \"\"\"Identifying information about code entities.\"\"\"\n",
    "    \n",
    "    entities: List = Field(\n",
    "        default=[],\n",
    "        description=\"Extracted programming specific named entities, such as named directories, \"\n",
    "        \"files, functions, classes, or imports in a single list (name matters only).\",\n",
    "    )\n",
    "\n",
    "def extract_code_entities(string_with_entities, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Extracts named entities from a given code string, including directories, files,  imports, function names, and class names.\n",
    "\n",
    "    This function uses a language model to analyze the provided text and extract named entities related to programming or coding\n",
    "\n",
    "    Args:\n",
    "        string_with_entities (str): Unstructured text as a string from which to extract entities.\n",
    "        model (str): The LLM model to use\n",
    "        \n",
    "    Returns:\n",
    "        entities: An instance of ProgrammingNamedEntities containing the extracted directories, files, imports, functions, and classes.\n",
    "    \"\"\"\n",
    "\n",
    "    llm=ChatOpenAI(\n",
    "        model_name=model\n",
    "    )\n",
    "    # Modify the prompt to focus on extracting code entities\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are extracting directories, files, imports, functions, and classes from the given text.\",\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Use the given format to extract information from the following input: {code_snippet}\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Set up the chain to extract the structured output\n",
    "    entity_chain = prompt | llm.with_structured_output(ProgrammingNamedEntities)\n",
    "\n",
    "    entities = entity_chain.invoke({'code_snippet': string_with_entities})\n",
    "\n",
    "    entities = entities.entities\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example usage\n",
    "test_entity_string = \"\"\"\n",
    "\"Hey, can you check the file src/utils/helpers.py? I think the process_data function is \n",
    "missing an import. You should import numpy and pandas at the beginning. Also, take a look \n",
    "at the DataProcessor class in models/data_processor.py. There's a bug in the transform_data \n",
    "method. Finally, the config/settings.json file might need an update to include \n",
    "the new API endpoint.\"\n",
    "\"\"\"\n",
    "\n",
    "programming_entities = extract_code_entities(string_with_entities=test_entity_string)\n",
    "print(programming_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from edoc.gpt_helpers.connect import connect_to_neo4j\n",
    "\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "URL = \"bolt://localhost:7687\"\n",
    "\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "    url=URL,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name= \"chunkSummaryVectorIndex\",\n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Chunk\",\n",
    "    text_node_properties=[\"id\", \"summary\"],\n",
    "    embedding_node_property=\"summary_embedding\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "What is this project about?\n",
    "\"\"\"\n",
    "\n",
    "top_n_unstructured_data = vector_index.similarity_search(question, k=3)\n",
    "top_n_unstructured_data = [item.page_content for item in top_n_unstructured_data]\n",
    "\n",
    "print(\n",
    "    '\\n'.join(top_n_unstructured_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extending this to work for any of our 4 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_index(vector_index_name, keyword_index_name, node_label, embedding_property, text_properties, model=\"text-embedding-3-small\", search_type=\"hybrid\"):\n",
    "    \"\"\"\n",
    "    Create a vector index for a given node label and embedding type.\n",
    "\n",
    "    Args:\n",
    "        vector_index_name (str): The name of the vector index we would like to use.\n",
    "        keyword_index_name (str) Keyword index name to use (created if run the first time)\n",
    "        node_label (str): The label of the nodes (e.g., 'Chunk', 'File', 'Directory').\n",
    "        embedding_property (str): The property name for the embeddings (e.g., 'summary_embedding', 'raw_embedding').\n",
    "        text_properties (list): List of text properties to include in the index (e.g., ['id', 'summary', 'raw_code']).\n",
    "        model (str): The OpenAI model to use. Default is 'text-embedding-3-small'.\n",
    "        search_type (str): The type of search ('hybrid', 'exact', etc.). Default is 'hybrid'.\n",
    "\n",
    "    Returns:\n",
    "        Neo4jVector: The vector index object.\n",
    "    \"\"\"\n",
    "    return Neo4jVector.from_existing_graph(\n",
    "        OpenAIEmbeddings(model=model),\n",
    "        url=URL,\n",
    "        username=NEO4J_USERNAME,\n",
    "        password=NEO4J_PASSWORD,\n",
    "        search_type=search_type,\n",
    "        index_name= vector_index_name,\n",
    "        keyword_index_name=keyword_index_name,\n",
    "        node_label=node_label,\n",
    "        text_node_properties=text_properties,\n",
    "        embedding_node_property=embedding_property\n",
    "    )\n",
    "\n",
    "def perform_similarity_search(vector_indexes, question, top_k=3):\n",
    "    \"\"\"\n",
    "    Perform a similarity search across one or more vector indexes.\n",
    "\n",
    "    Args:\n",
    "        vector_indexes (list): A list of Neo4jVector objects to search.\n",
    "        question (str): The search query.\n",
    "        top_k (int): The number of top results to return. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of top results across all vector indexes.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for vector_index in vector_indexes:\n",
    "        top_n_data = vector_index.similarity_search(question, k=top_k)\n",
    "        results.extend(top_n_data)\n",
    "\n",
    "    return [item.page_content for item in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector indexes\n",
    "chunk_summary_index = create_vector_index(\"chunkSummaryVectorIndex\", \"code_summary_keyword\", \"Chunk\", \"summary_embedding\", [\"id\", \"summary\"])\n",
    "chunk_raw_index = create_vector_index(\"chunkRawVectorIndex\", \"code_raw_keyword\", \"Chunk\", \"chunk_embedding\", [\"id\", \"raw_code\"])\n",
    "file_summary_index = create_vector_index(\"fileSummaryVectorIndex\", \"file_keyword\", \"File\", \"summary_embedding\", [\"path\", \"summary\"])\n",
    "dir_summary_index = create_vector_index(\"dirSummaryVectorIndex\", \"dir_keyword\", \"Directory\", \"summary_embedding\", [\"path\", \"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Search in chunk summaries and raw code\n",
    "vector_indexes = [chunk_summary_index, chunk_raw_index]\n",
    "results = perform_similarity_search(vector_indexes, \"Can you tell me more about `ResumeSection`?\", top_k=2)\n",
    "\n",
    "print(\n",
    "    '\\n'.join(results)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Search across all available summaries (chunk, file, directory)\n",
    "vector_indexes = [chunk_summary_index, file_summary_index, dir_summary_index]\n",
    "results = perform_similarity_search(vector_indexes, \"Summarize the contents of the project please\", top_k=2)\n",
    "\n",
    "\n",
    "print(\n",
    "    '\\n'.join(results)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Search only in directory summaries\n",
    "vector_indexes = [dir_summary_index]\n",
    "results = perform_similarity_search(vector_indexes, \"Summarize the file system from the top\", top_k=3)\n",
    "\n",
    "\n",
    "print(\n",
    "    '\\n'.join(results)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edoc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
