{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create base graph by walking through the directory and finding files and dirs, adding them to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "from edoc.connect import connect_to_neo4j\n",
    "\n",
    "class CodebaseGraph:\n",
    "    def __init__(self, root_directory, uri=\"bolt://localhost:7687\", user=None, password=None, openai_api_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the CodebaseGraph with a connection to Neo4j.\n",
    "\n",
    "        Args:\n",
    "            uri (str): The URI of the Neo4j database. Defaults to localhost.\n",
    "            user (str): Username for Neo4j. If None, loads from environment variable NEO4J_USERNAME.\n",
    "            password (str): Password for Neo4j. If None, loads from environment variable NEO4J_PASSWORD.\n",
    "            openai_api_key (str): Key needed to access OpenAI API\n",
    "        \"\"\"\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "\n",
    "        # Set up the Neo4j connection\n",
    "        self.uri = uri\n",
    "        self.NEO4J_USER = user or os.getenv(\"NEO4J_USERNAME\")\n",
    "        self.NEO4J_PASSWORD =  password or os.getenv(\"NEO4J_PASSWORD\")\n",
    "        self.OPENAI_API_KEY = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        self.root_directory = root_directory\n",
    "\n",
    "        if not self.NEO4J_USER or not self.NEO4J_PASSWORD:\n",
    "            raise ValueError(\"NEO4J_USERNAME and NEO4J_PASSWORD must be provided either as arguments or environment variables.\")\n",
    "        \n",
    "        if not self.OPENAI_API_KEY:\n",
    "            raise ValueError(\"NEO4J_USERNAME and NEO4J_PASSWORD must be provided either as arguments or environment variables.\")\n",
    "\n",
    "        self.kg = connect_to_neo4j()\n",
    "\n",
    "    def get_file_info(self, file_path):\n",
    "        \"\"\"\n",
    "        Get information about a file, including type, size, last modified date, creation date, permissions, owner, and hash.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing file information.\n",
    "        \"\"\"\n",
    "        stats = os.stat(file_path)\n",
    "        file_type = os.path.splitext(file_path)[1][1:]  # Get file extension without the dot\n",
    "        size = stats.st_size\n",
    "        last_modified = datetime.fromtimestamp(stats.st_mtime).isoformat()\n",
    "        created = datetime.fromtimestamp(stats.st_ctime).isoformat()\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"type\": file_type,\n",
    "            \"size\": size,\n",
    "            \"last_modified\": last_modified,\n",
    "            \"created\": created,\n",
    "        }\n",
    "\n",
    "    def _load_dirs_and_files_to_graph(self):\n",
    "        \"\"\"\n",
    "        Traverse a directory and create a graph in Neo4j representing the directory structure and file information.\n",
    "\n",
    "        Args:\n",
    "            root_directory (str): The root directory to start traversing from.\n",
    "        \"\"\"\n",
    "        for root, dirs, files in os.walk(self.root_directory):\n",
    "\n",
    "            dir_name = os.path.basename(root)\n",
    "\n",
    "            # Create node for the directory\n",
    "            self.kg.query(\n",
    "                \"\"\"\n",
    "                MERGE (dir:Directory {name: $dir_name, path: $path})\n",
    "                ON CREATE SET dir.created = $created, dir.last_modified = $last_modified\n",
    "                \"\"\",\n",
    "                {\n",
    "                    'dir_name':dir_name,\n",
    "                    'path':root,\n",
    "                    'created':datetime.fromtimestamp(os.stat(root).st_ctime).isoformat(),\n",
    "                    'last_modified':datetime.fromtimestamp(os.stat(root).st_mtime).isoformat(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Create nodes for subdirectories\n",
    "            for dir_name in dirs:\n",
    "                dir_path = os.path.join(root, dir_name)\n",
    "                self.kg.query(\n",
    "                    \"\"\"\n",
    "                        MERGE (subdir:Directory {name: $dir_name, path: $subdir_path})\n",
    "                        ON CREATE SET subdir.created = $created, subdir.last_modified = $last_modified\n",
    "                        WITH subdir\n",
    "                        MATCH (parent:Directory {path: $parent_path})\n",
    "                        MERGE (parent)-[:CONTAINS]->(subdir)\n",
    "                    \"\"\",\n",
    "                    {\n",
    "                        'dir_name':dir_name,\n",
    "                        'subdir_path':dir_path, \n",
    "                        'parent_path':root,\n",
    "                        'created':datetime.fromtimestamp(os.stat(dir_path).st_ctime).isoformat(),\n",
    "                        'last_modified':datetime.fromtimestamp(os.stat(dir_path).st_mtime).isoformat(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Create nodes for files\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                file_info = self.get_file_info(file_path)\n",
    "                self.kg.query(\n",
    "                    \"\"\"\n",
    "                        MERGE (file:File {name: $file_name, path: $file_path})\n",
    "                        ON CREATE SET file.type = $type, file.size = $size, file.last_modified = $last_modified, file.created = $created\n",
    "                        WITH file\n",
    "                        MATCH (parent:Directory {path: $parent_path})\n",
    "                        MERGE (parent)-[:CONTAINS]->(file)\n",
    "                    \"\"\",\n",
    "                    {\n",
    "                        'file_name':file_name, \n",
    "                        'file_path':file_path, \n",
    "                        'type':file_info['type'], \n",
    "                        'size':file_info['size'], \n",
    "                        'last_modified':file_info['last_modified'], \n",
    "                        'created':file_info['created'], \n",
    "                        'parent_path':root\n",
    "                    }\n",
    "                )\n",
    "        \n",
    "    def create_graph(self):\n",
    "\n",
    "        self._load_dirs_and_files_to_graph()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to write data to connection IPv4Address(('localhost', 7687)) (ResolvedIPv6Address(('::1', 7687, 0, 0)))\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "codebase_graph = CodebaseGraph(\n",
    "    root_directory=\"C:\\\\Users\\\\willd\\\\Documents\\\\Git\\\\graphRag\\\\test_project\"\n",
    ")\n",
    "codebase_graph.create_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "Directory {last_modified: STRING, created: STRING, path: STRING, name: STRING}\n",
      "File {name: STRING, type: STRING, size: INTEGER, last_modified: STRING, path: STRING, created: STRING}\n",
      "Chunk {raw_code: STRING, id: STRING, summary: STRING, summary_embedding: LIST, chunk_embedding: LIST}\n",
      "Import {module: STRING, entities: LIST}\n",
      "Function {parameters: STRING, name: STRING}\n",
      "Relationship properties:\n",
      "\n",
      "The relationships:\n",
      "(:Directory)-[:CONTAINS]->(:File)\n",
      "(:Directory)-[:CONTAINS]->(:Directory)\n",
      "(:File)-[:CONTAINS]->(:Chunk)\n",
      "(:File)-[:DEFINES]->(:Function)\n",
      "(:File)-[:CALLS]->(:Import)\n"
     ]
    }
   ],
   "source": [
    "print(codebase_graph.kg.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next work on file enrichments. We will need to create functions to connect to opneai api as well as functions to perform embeddings and extractoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "def create_chat_completion(messages, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Create a chat completion using the OpenAI API.\n",
    "\n",
    "    Args:\n",
    "        messages (list): A list of message dictionaries for the conversation.\n",
    "        model (str): The OpenAI model to use. Default is 'gpt-4o-mini'.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the response from the OpenAI API.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    response = client.chat.completions.create(messages=messages, model=model)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def summarize_file_chunk(chunk_text, file_name, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Summarize a chunk of text from a file using OpenAI's language model.\n",
    "\n",
    "    Args:\n",
    "        chunk_text (str): The text chunk to summarize.\n",
    "        file_name (str): The name of the file from which the chunk was extracted.\n",
    "        model (str): The OpenAI model to use. Default is 'gpt-4o-mini'.\n",
    "\n",
    "    Returns:\n",
    "        str: A brief and clear summary of the chunk.\n",
    "    \"\"\"\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"I have this text from a file named {file_name}. The text is:\\n{chunk_text}\\nPlease summarize it in simple terms. Try to keep the summary brief, but maintain clarity.\"}\n",
    "    ]\n",
    "    return create_chat_completion(messages=prompt, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = \"utils.py\"\n",
    "test_chunk = \"\"\"\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def process_file(input_path, output_path):\n",
    "    with open(input_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(content.upper())\n",
    "\n",
    "    print(f\"Processed {{input_path}} and saved results to {{output_path}}\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `utils.py` file defines a function called `process_file`, which takes an input file path and an output file path. It reads the content of the input file, converts it to uppercase, and writes the modified content to the output file. After processing, it prints a message indicating the input and output paths. The file also imports the `os` module and the `load_dotenv` function from the `dotenv` package.\n"
     ]
    }
   ],
   "source": [
    "test_summary = summarize_file_chunk(file_name=test_file_name, chunk_text=test_chunk)\n",
    "print(test_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Up next we will create embeddings from a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Generate an embedding for a given text using OpenAI's embedding model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be embedded. Newlines are replaced with spaces.\n",
    "        model (str): The OpenAI model to use. Default is 'text-embedding-3-small'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of floats representing the embedding vector of the input text.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embedding = get_embedding(text=test_summary)\n",
    "len(test_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onto creating a structured output to use with llms. The goal should be to take code and extract imports, functions, and classes. We use [this](https://medium.com/neo4j/enhancing-the-accuracy-of-rag-applications-with-knowledge-graphs-ad5e2ffab663) for insperation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm=ChatOpenAI(\n",
    "    model_name='gpt-4o-mini'\n",
    ")\n",
    "\n",
    "class Parameter(BaseModel):\n",
    "    \"\"\"Model representing a function or class parameter.\"\"\"\n",
    "    name: str\n",
    "    type: str\n",
    "\n",
    "class FunctionEntity(BaseModel):\n",
    "    \"\"\"Model representing a function entity.\"\"\"\n",
    "    name: str\n",
    "    parameters: List[Parameter]\n",
    "    return_type: Optional[str] = None\n",
    "\n",
    "class ClassEntity(BaseModel):\n",
    "    \"\"\"Model representing a class entity.\"\"\"\n",
    "    name: str\n",
    "    parameters: List[Parameter]\n",
    "\n",
    "class ImportEntity(BaseModel):\n",
    "    \"\"\"Model representing an import entity.\"\"\"\n",
    "    module: str\n",
    "    entities: List[str]\n",
    "\n",
    "class CodeEntities(BaseModel):\n",
    "    \"\"\"Identifying information about code entities.\"\"\"\n",
    "    \n",
    "    imports: List[ImportEntity] = Field(\n",
    "        default=[],\n",
    "        description=\"All the import statements in the code, with the module \"\n",
    "        \"and specific entities being imported.\",\n",
    "    )\n",
    "    functions: List[FunctionEntity] = Field(\n",
    "        default=[],\n",
    "        description=\"All the function names in the code, including their parameters, returns, \"\n",
    "        \"and types if available.\",\n",
    "    )\n",
    "    classes: List[ClassEntity] = Field(\n",
    "        default=[],\n",
    "        description=\"All the class names in the code, including their parameters \"\n",
    "        \"and types if available.\",\n",
    "    )\n",
    "\n",
    "def extract_code_entities(code_string):\n",
    "    \"\"\"\n",
    "    Extracts code entities from a given code string, including imports, function names, and class names.\n",
    "\n",
    "    This function uses a language model to analyze the provided code string and extract relevant code entities. \n",
    "    For imports, it identifies the module and specific entities being imported. For functions and classes, it \n",
    "    captures their names, parameters, and types if available.\n",
    "\n",
    "    Args:\n",
    "        code_string (str): The code snippet as a string from which to extract entities.\n",
    "\n",
    "    Returns:\n",
    "        entities: An instance of CodeEntities containing the extracted imports, functions, and classes.\n",
    "    \"\"\"\n",
    "    # Modify the prompt to focus on extracting code entities\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are extracting imports, function names, and class names from the given code. \"\n",
    "                \"For imports, provide the module and specific entities being imported. \"\n",
    "                \"For functions and classes, include their parameters and types if available.\",\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Use the given format to extract information from the following input: {code_snippet}\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Set up the chain to extract the structured output\n",
    "    entity_chain = prompt | llm.with_structured_output(CodeEntities)\n",
    "\n",
    "    entities = entity_chain.invoke({'code_snippet': code_string})\n",
    "\n",
    "    entities = entities.dict()\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_example = \"\"\"\n",
    "import java.util.List;\n",
    "import java.util.Optional;\n",
    "\n",
    "public class ExampleClass {\n",
    "    private String param1;\n",
    "    private int param2;\n",
    "\n",
    "    public ExampleClass(String param1, int param2) {\n",
    "        this.param1 = param1;\n",
    "        this.param2 = param2;\n",
    "    }\n",
    "\n",
    "    public boolean exampleMethod(List<String> arg1, Optional<Integer> arg2) {\n",
    "        return arg2.isPresent();\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imports': [{'module': 'java.util', 'entities': ['List', 'Optional']}],\n",
       " 'functions': [{'name': 'exampleMethod',\n",
       "   'parameters': [{'name': 'arg1', 'type': 'List<String>'},\n",
       "    {'name': 'arg2', 'type': 'Optional<Integer>'}],\n",
       "   'return_type': 'boolean'}],\n",
       " 'classes': [{'name': 'ExampleClass',\n",
       "   'parameters': [{'name': 'param1', 'type': 'String'},\n",
       "    {'name': 'param2', 'type': 'int'}]}]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_entities = extract_code_entities(entity_example)\n",
    "\n",
    "test_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now prepare to chunk files to be iterated through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "\n",
    "text_splitter = PythonCodeTextSplitter(chunk_size=500, chunk_overlap=25)\n",
    "chunks = text_splitter.split_text(entity_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['import java.util.List;\\nimport java.util.Optional;\\n\\npublic class ExampleClass {\\n    private String param1;\\n    private int param2;\\n\\n    public ExampleClass(String param1, int param2) {\\n        this.param1 = param1;\\n        this.param2 = param2;\\n    }\\n\\n    public boolean exampleMethod(List<String> arg1, Optional<Integer> arg2) {\\n        return arg2.isPresent();\\n    }\\n}']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_contents(file_path):\n",
    "    \"\"\"\n",
    "    Opens a file and reads its contents as text.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to be read.\n",
    "\n",
    "    Returns:\n",
    "        str: The contents of the file as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            contents = file.read()\n",
    "        return contents\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "from edoc.connect import connect_to_neo4j\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "\n",
    "\n",
    "class CodebaseGraph:\n",
    "    def __init__(\n",
    "            self, \n",
    "            root_directory, \n",
    "            uri=\"bolt://localhost:7687\", \n",
    "            user=None, \n",
    "            password=None, \n",
    "            openai_api_key=None,\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=25\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the CodebaseGraph with a connection to Neo4j.\n",
    "\n",
    "        Args:\n",
    "            uri (str): The URI of the Neo4j database. Defaults to localhost.\n",
    "            user (str): Username for Neo4j. If None, loads from environment variable NEO4J_USERNAME.\n",
    "            password (str): Password for Neo4j. If None, loads from environment variable NEO4J_PASSWORD.\n",
    "            openai_api_key (str): Key needed to access OpenAI API\n",
    "        \"\"\"\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "\n",
    "        # Set up the Neo4j connection\n",
    "        self.uri = uri\n",
    "        self.NEO4J_USER = user or os.getenv(\"NEO4J_USERNAME\")\n",
    "        self.NEO4J_PASSWORD =  password or os.getenv(\"NEO4J_PASSWORD\")\n",
    "        self.OPENAI_API_KEY = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        self.root_directory = root_directory\n",
    "\n",
    "        if not self.NEO4J_USER or not self.NEO4J_PASSWORD:\n",
    "            raise ValueError(\"NEO4J_USERNAME and NEO4J_PASSWORD must be provided either as arguments or environment variables.\")\n",
    "        \n",
    "        if not self.OPENAI_API_KEY:\n",
    "            raise ValueError(\"NEO4J_USERNAME and NEO4J_PASSWORD must be provided either as arguments or environment variables.\")\n",
    "\n",
    "        self.kg = connect_to_neo4j()\n",
    "\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "\n",
    "    def get_file_info(self, file_path):\n",
    "        \"\"\"\n",
    "        Get information about a file, including type, size, last modified date, creation date, permissions, owner, and hash.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing file information.\n",
    "        \"\"\"\n",
    "        stats = os.stat(file_path)\n",
    "        file_type = os.path.splitext(file_path)[1][1:]  # Get file extension without the dot\n",
    "        size = stats.st_size\n",
    "        last_modified = datetime.fromtimestamp(stats.st_mtime).isoformat()\n",
    "        created = datetime.fromtimestamp(stats.st_ctime).isoformat()\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"type\": file_type,\n",
    "            \"size\": size,\n",
    "            \"last_modified\": last_modified,\n",
    "            \"created\": created,\n",
    "        }\n",
    "    \n",
    "    def read_file_contents(self, file_path):\n",
    "        \"\"\"\n",
    "        Opens a file and reads its contents as text.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file to be read.\n",
    "\n",
    "        Returns:\n",
    "            str: The contents of the file as a string.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                contents = file.read()\n",
    "            return contents\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading the file: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _load_dirs_and_files_to_graph(self):\n",
    "        \"\"\"\n",
    "        Traverse a directory and create a graph in Neo4j representing the directory structure and file information.\n",
    "\n",
    "        Args:\n",
    "            root_directory (str): The root directory to start traversing from.\n",
    "        \"\"\"\n",
    "        for root, dirs, files in os.walk(self.root_directory):\n",
    "\n",
    "            dir_name = os.path.basename(root)\n",
    "\n",
    "            # Create node for the directory\n",
    "            self.kg.query(\n",
    "                \"\"\"\n",
    "                MERGE (dir:Directory {name: $dir_name, path: $path})\n",
    "                ON CREATE SET dir.created = $created, dir.last_modified = $last_modified\n",
    "                \"\"\",\n",
    "                {\n",
    "                    'dir_name':dir_name,\n",
    "                    'path':root,\n",
    "                    'created':datetime.fromtimestamp(os.stat(root).st_ctime).isoformat(),\n",
    "                    'last_modified':datetime.fromtimestamp(os.stat(root).st_mtime).isoformat(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Create nodes for subdirectories\n",
    "            for dir_name in dirs:\n",
    "                dir_path = os.path.join(root, dir_name)\n",
    "                self.kg.query(\n",
    "                    \"\"\"\n",
    "                        MERGE (subdir:Directory {name: $dir_name, path: $subdir_path})\n",
    "                        ON CREATE SET subdir.created = $created, subdir.last_modified = $last_modified\n",
    "                        WITH subdir\n",
    "                        MATCH (parent:Directory {path: $parent_path})\n",
    "                        MERGE (parent)-[:CONTAINS]->(subdir)\n",
    "                    \"\"\",\n",
    "                    {\n",
    "                        'dir_name':dir_name,\n",
    "                        'subdir_path':dir_path, \n",
    "                        'parent_path':root,\n",
    "                        'created':datetime.fromtimestamp(os.stat(dir_path).st_ctime).isoformat(),\n",
    "                        'last_modified':datetime.fromtimestamp(os.stat(dir_path).st_mtime).isoformat(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Create nodes for files\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                file_info = self.get_file_info(file_path)\n",
    "                self.kg.query(\n",
    "                    \"\"\"\n",
    "                        MERGE (file:File {name: $file_name, path: $file_path})\n",
    "                        ON CREATE SET file.type = $type, file.size = $size, file.last_modified = $last_modified, file.created = $created\n",
    "                        WITH file\n",
    "                        MATCH (parent:Directory {path: $parent_path})\n",
    "                        MERGE (parent)-[:CONTAINS]->(file)\n",
    "                    \"\"\",\n",
    "                    {\n",
    "                        'file_name':file_name, \n",
    "                        'file_path':file_path, \n",
    "                        'type':file_info['type'], \n",
    "                        'size':file_info['size'], \n",
    "                        'last_modified':file_info['last_modified'], \n",
    "                        'created':file_info['created'], \n",
    "                        'parent_path':root\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    def _enrich_graph(self):\n",
    "        \"\"\"\n",
    "        Enriches the knowledge graph by processing files, creating and linking code chunks, and extracting unique code entities.\n",
    "        \"\"\"\n",
    "        text_splitter = PythonCodeTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "\n",
    "        # Query to find files without chunk nodes\n",
    "        query = \"\"\"\n",
    "        MATCH (file:File)\n",
    "        WHERE NOT (file)-[:CONTAINS]->(:Chunk)\n",
    "        RETURN file.path AS file_path\n",
    "        \"\"\"\n",
    "\n",
    "        result = self.kg.query(query)\n",
    "        file_paths = [record['file_path'] for record in result]\n",
    "\n",
    "        for file in tqdm(file_paths):\n",
    "            file_contents = self.read_file_contents(file)\n",
    "\n",
    "            if file_contents is not None:\n",
    "                chunks = text_splitter.split_text(file_contents)\n",
    "\n",
    "                # Initialize collections for unique entities\n",
    "                unique_imports = {}\n",
    "                unique_functions = {}\n",
    "                unique_classes = {}\n",
    "\n",
    "                for idx, chunk in enumerate(chunks):\n",
    "                    chunk_id = f\"{file}_chunk_{idx}\"\n",
    "                    chunk_summary = summarize_file_chunk(chunk_text=chunk, file_name=file)\n",
    "                    summary_embedding = get_embedding(chunk_summary)\n",
    "                    chunk_embedding = get_embedding(chunk)\n",
    "\n",
    "                    # Create the chunk node and link it to the file\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (chunk:Chunk {id: $chunk_id})\n",
    "                        SET chunk.raw_code = $raw_code, \n",
    "                            chunk.summary = $summary, \n",
    "                            chunk.summary_embedding = $summary_embedding, \n",
    "                            chunk.chunk_embedding = $chunk_embedding\n",
    "                        WITH chunk\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:CONTAINS]->(chunk)\n",
    "                    \"\"\", {\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'raw_code': chunk,\n",
    "                        'summary': chunk_summary,\n",
    "                        'summary_embedding': summary_embedding,\n",
    "                        'chunk_embedding': chunk_embedding,\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                    # Extract and process code entities for each chunk\n",
    "                    chunk_entities = extract_code_entities(chunk)\n",
    "\n",
    "                    # Collect unique imports\n",
    "                    for imp in chunk_entities['imports']:\n",
    "                        module_name = imp['module']\n",
    "                        if module_name not in unique_imports:\n",
    "                            unique_imports[module_name] = set(imp['entities'])\n",
    "                        else:\n",
    "                            unique_imports[module_name].update(imp['entities'])\n",
    "\n",
    "                    # Collect unique functions\n",
    "                    for func in chunk_entities['functions']:\n",
    "                        func_name = func['name']\n",
    "                        if func_name not in unique_functions:\n",
    "                            unique_functions[func_name] = {\n",
    "                                'parameters': json.dumps([{'name': param['name'], 'type': param['type']} for param in func['parameters']]),\n",
    "                                'return_type': func['return_type']\n",
    "                            }\n",
    "\n",
    "                    # Collect unique classes\n",
    "                    for cls in chunk_entities['classes']:\n",
    "                        cls_name = cls['name']\n",
    "                        if cls_name not in unique_classes:\n",
    "                            unique_classes[cls_name] = {\n",
    "                                'parameters': json.dumps([{'name': param['name'], 'type': param['type']} for param in cls['parameters']])\n",
    "                            }\n",
    "\n",
    "                # Store unique entities in the graph\n",
    "\n",
    "                # Create and link unique import nodes\n",
    "                for module, entities in unique_imports.items():\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (import:Import {module: $module})\n",
    "                        SET import.entities = $entities\n",
    "                        WITH import\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:CALLS]->(import)\n",
    "                    \"\"\", {\n",
    "                        'module': module,\n",
    "                        'entities': list(entities),\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                # Create and link unique function nodes\n",
    "                for name, func in unique_functions.items():\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (function:Function {name: $name})\n",
    "                        SET function.parameters = $parameters, function.return_type = $return_type\n",
    "                        WITH function\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:DEFINES]->(function)\n",
    "                    \"\"\", {\n",
    "                        'name': name,\n",
    "                        'parameters': func['parameters'],\n",
    "                        'return_type': func['return_type'],\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                # Create and link unique class nodes\n",
    "                for name, cls in unique_classes.items():\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (class:Class {name: $name})\n",
    "                        SET class.parameters = $parameters\n",
    "                        WITH class\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:DEFINES]->(class)\n",
    "                    \"\"\", {\n",
    "                        'name': name,\n",
    "                        'parameters': cls['parameters'],\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                # Link all chunks in sequence using APOC's `NEXT` relationship\n",
    "                self.kg.query(\"\"\"\n",
    "                    MATCH (file:File {path: $file_path})-[:CONTAINS]->(chunk:Chunk)\n",
    "                    WITH chunk ORDER BY chunk.id ASC\n",
    "                    WITH collect(chunk) AS chunks\n",
    "                    CALL apoc.nodes.link(chunks, 'NEXT')\n",
    "                    RETURN count(*)\n",
    "                \"\"\", {\n",
    "                    'file_path': file\n",
    "                })\n",
    "\n",
    "\n",
    "    def create_graph(self):\n",
    "\n",
    "        self._load_dirs_and_files_to_graph()\n",
    "\n",
    "        self._enrich_graph()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:17<00:00,  2.15s/it]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "codebase_graph = CodebaseGraph(\n",
    "    root_directory=\"C:\\\\Users\\\\willd\\\\Documents\\\\Git\\\\graphRag\\\\test_project\"\n",
    ")\n",
    "codebase_graph.create_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final step will be to go through the graph and look at files with summaries, and then use those summaries to summarize files. This aggregation procecss can be used for all files to summarize directories. After this we need to create vector index for the code snippets and the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_list_of_summaries(summaries, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Summarize a chunk of text from a file using OpenAI's language model.\n",
    "\n",
    "    Args:\n",
    "        summaries (lst[str]): The text chunk to summarize.\n",
    "        model (str): The OpenAI model to use. Default is 'gpt-4o-mini'.\n",
    "\n",
    "    Returns:\n",
    "        str: A brief and clear summary of the chunk.\n",
    "    \"\"\"\n",
    "    context = '\\n'.join(summaries)\n",
    "\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"\"\"I would like you to summarize some information for me. The goal is to condense understanding across a file system.\n",
    "            It may include chunk summaries (which are smaller sections of a file), file summaries, or subdirectory summareis. \n",
    "            The goal is to gain global understanding by merging themes at higher levels.\n",
    "            Please use the given context to make a summary, no need to confirm. Simply return simple and brief summaries based on context. The context is: {context}.\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    return create_chat_completion(messages=prompt, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "from edoc.connect import connect_to_neo4j\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "\n",
    "\n",
    "class CodebaseGraph:\n",
    "    def __init__(\n",
    "            self, \n",
    "            root_directory, \n",
    "            uri=\"bolt://localhost:7687\", \n",
    "            user=None, \n",
    "            password=None, \n",
    "            openai_api_key=None,\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=25\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the CodebaseGraph with a connection to Neo4j.\n",
    "\n",
    "        Args:\n",
    "            uri (str): The URI of the Neo4j database. Defaults to localhost.\n",
    "            user (str): Username for Neo4j. If None, loads from environment variable NEO4J_USERNAME.\n",
    "            password (str): Password for Neo4j. If None, loads from environment variable NEO4J_PASSWORD.\n",
    "            openai_api_key (str): Key needed to access OpenAI API\n",
    "        \"\"\"\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "\n",
    "        # Set up the Neo4j connection\n",
    "        self.uri = uri\n",
    "        self.NEO4J_USER = user or os.getenv(\"NEO4J_USERNAME\")\n",
    "        self.NEO4J_PASSWORD =  password or os.getenv(\"NEO4J_PASSWORD\")\n",
    "        self.OPENAI_API_KEY = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        self.root_directory = root_directory\n",
    "\n",
    "        if not self.NEO4J_USER or not self.NEO4J_PASSWORD:\n",
    "            raise ValueError(\"NEO4J_USERNAME and NEO4J_PASSWORD must be provided either as arguments or environment variables.\")\n",
    "        \n",
    "        if not self.OPENAI_API_KEY:\n",
    "            raise ValueError(\"NEO4J_USERNAME and NEO4J_PASSWORD must be provided either as arguments or environment variables.\")\n",
    "\n",
    "        self.kg = connect_to_neo4j()\n",
    "\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "\n",
    "    def get_file_info(self, file_path):\n",
    "        \"\"\"\n",
    "        Get information about a file, including type, size, last modified date, creation date, permissions, owner, and hash.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing file information.\n",
    "        \"\"\"\n",
    "        stats = os.stat(file_path)\n",
    "        file_type = os.path.splitext(file_path)[1][1:]  # Get file extension without the dot\n",
    "        size = stats.st_size\n",
    "        last_modified = datetime.fromtimestamp(stats.st_mtime).isoformat()\n",
    "        created = datetime.fromtimestamp(stats.st_ctime).isoformat()\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"type\": file_type,\n",
    "            \"size\": size,\n",
    "            \"last_modified\": last_modified,\n",
    "            \"created\": created,\n",
    "        }\n",
    "    \n",
    "    def read_file_contents(self, file_path):\n",
    "        \"\"\"\n",
    "        Opens a file and reads its contents as text.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file to be read.\n",
    "\n",
    "        Returns:\n",
    "            str: The contents of the file as a string.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                contents = file.read()\n",
    "            return contents\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading the file: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _load_dirs_and_files_to_graph(self):\n",
    "        \"\"\"\n",
    "        Traverse a directory and create a graph in Neo4j representing the directory structure and file information.\n",
    "\n",
    "        Args:\n",
    "            root_directory (str): The root directory to start traversing from.\n",
    "        \"\"\"\n",
    "        for root, dirs, files in os.walk(self.root_directory):\n",
    "\n",
    "            dir_name = os.path.basename(root)\n",
    "\n",
    "            # Create node for the directory\n",
    "            self.kg.query(\n",
    "                \"\"\"\n",
    "                MERGE (dir:Directory {name: $dir_name, path: $path})\n",
    "                ON CREATE SET dir.created = $created, dir.last_modified = $last_modified\n",
    "                \"\"\",\n",
    "                {\n",
    "                    'dir_name':dir_name,\n",
    "                    'path':root,\n",
    "                    'created':datetime.fromtimestamp(os.stat(root).st_ctime).isoformat(),\n",
    "                    'last_modified':datetime.fromtimestamp(os.stat(root).st_mtime).isoformat(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Create nodes for subdirectories\n",
    "            for dir_name in dirs:\n",
    "                dir_path = os.path.join(root, dir_name)\n",
    "                self.kg.query(\n",
    "                    \"\"\"\n",
    "                        MERGE (subdir:Directory {name: $dir_name, path: $subdir_path})\n",
    "                        ON CREATE SET subdir.created = $created, subdir.last_modified = $last_modified\n",
    "                        WITH subdir\n",
    "                        MATCH (parent:Directory {path: $parent_path})\n",
    "                        MERGE (parent)-[:CONTAINS]->(subdir)\n",
    "                    \"\"\",\n",
    "                    {\n",
    "                        'dir_name':dir_name,\n",
    "                        'subdir_path':dir_path, \n",
    "                        'parent_path':root,\n",
    "                        'created':datetime.fromtimestamp(os.stat(dir_path).st_ctime).isoformat(),\n",
    "                        'last_modified':datetime.fromtimestamp(os.stat(dir_path).st_mtime).isoformat(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Create nodes for files\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                file_info = self.get_file_info(file_path)\n",
    "                self.kg.query(\n",
    "                    \"\"\"\n",
    "                        MERGE (file:File {name: $file_name, path: $file_path})\n",
    "                        ON CREATE SET file.type = $type, file.size = $size, file.last_modified = $last_modified, file.created = $created\n",
    "                        WITH file\n",
    "                        MATCH (parent:Directory {path: $parent_path})\n",
    "                        MERGE (parent)-[:CONTAINS]->(file)\n",
    "                    \"\"\",\n",
    "                    {\n",
    "                        'file_name':file_name, \n",
    "                        'file_path':file_path, \n",
    "                        'type':file_info['type'], \n",
    "                        'size':file_info['size'], \n",
    "                        'last_modified':file_info['last_modified'], \n",
    "                        'created':file_info['created'], \n",
    "                        'parent_path':root\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    def _enrich_graph(self):\n",
    "        \"\"\"\n",
    "        Enriches the knowledge graph by processing files, creating and linking code chunks, and extracting unique code entities.\n",
    "        \"\"\"\n",
    "        text_splitter = PythonCodeTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "\n",
    "        # Query to find files without chunk nodes\n",
    "        query = \"\"\"\n",
    "        MATCH (file:File)\n",
    "        WHERE NOT (file)-[:CONTAINS]->(:Chunk)\n",
    "        RETURN file.path AS file_path\n",
    "        \"\"\"\n",
    "\n",
    "        result = self.kg.query(query)\n",
    "        file_paths = [record['file_path'] for record in result]\n",
    "\n",
    "        for file in tqdm(file_paths, desc='Creating chunks'):\n",
    "            file_contents = self.read_file_contents(file)\n",
    "\n",
    "            if file_contents is not None:\n",
    "                chunks = text_splitter.split_text(file_contents)\n",
    "\n",
    "                # Initialize collections for unique entities\n",
    "                unique_imports = {}\n",
    "                unique_functions = {}\n",
    "                unique_classes = {}\n",
    "\n",
    "                for idx, chunk in enumerate(chunks):\n",
    "                    chunk_id = f\"{file}_chunk_{idx}\"\n",
    "                    chunk_summary = summarize_file_chunk(chunk_text=chunk, file_name=file)\n",
    "                    summary_embedding = get_embedding(chunk_summary)\n",
    "                    chunk_embedding = get_embedding(chunk)\n",
    "\n",
    "                    # Create the chunk node and link it to the file\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (chunk:Chunk {id: $chunk_id})\n",
    "                        SET chunk.raw_code = $raw_code, \n",
    "                            chunk.summary = $summary, \n",
    "                            chunk.summary_embedding = $summary_embedding, \n",
    "                            chunk.chunk_embedding = $chunk_embedding\n",
    "                        WITH chunk\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:CONTAINS]->(chunk)\n",
    "                    \"\"\", {\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'raw_code': chunk,\n",
    "                        'summary': chunk_summary,\n",
    "                        'summary_embedding': summary_embedding,\n",
    "                        'chunk_embedding': chunk_embedding,\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                    # Extract and process code entities for each chunk\n",
    "                    chunk_entities = extract_code_entities(chunk)\n",
    "\n",
    "                    # Collect unique imports\n",
    "                    for imp in chunk_entities['imports']:\n",
    "                        module_name = imp['module']\n",
    "                        if module_name not in unique_imports:\n",
    "                            unique_imports[module_name] = set(imp['entities'])\n",
    "                        else:\n",
    "                            unique_imports[module_name].update(imp['entities'])\n",
    "\n",
    "                    # Collect unique functions\n",
    "                    for func in chunk_entities['functions']:\n",
    "                        func_name = func['name']\n",
    "                        if func_name not in unique_functions:\n",
    "                            unique_functions[func_name] = {\n",
    "                                'parameters': json.dumps([{'name': param['name'], 'type': param['type']} for param in func['parameters']]),\n",
    "                                'return_type': func['return_type']\n",
    "                            }\n",
    "\n",
    "                    # Collect unique classes\n",
    "                    for cls in chunk_entities['classes']:\n",
    "                        cls_name = cls['name']\n",
    "                        if cls_name not in unique_classes:\n",
    "                            unique_classes[cls_name] = {\n",
    "                                'parameters': json.dumps([{'name': param['name'], 'type': param['type']} for param in cls['parameters']])\n",
    "                            }\n",
    "\n",
    "                # Store unique entities in the graph\n",
    "\n",
    "                # Create and link unique import nodes\n",
    "                for module, entities in unique_imports.items():\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (import:Import {module: $module})\n",
    "                        SET import.entities = $entities\n",
    "                        WITH import\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:CALLS]->(import)\n",
    "                    \"\"\", {\n",
    "                        'module': module,\n",
    "                        'entities': list(entities),\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                # Create and link unique function nodes\n",
    "                for name, func in unique_functions.items():\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (function:Function {name: $name})\n",
    "                        SET function.parameters = $parameters, function.return_type = $return_type\n",
    "                        WITH function\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:DEFINES]->(function)\n",
    "                    \"\"\", {\n",
    "                        'name': name,\n",
    "                        'parameters': func['parameters'],\n",
    "                        'return_type': func['return_type'],\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                # Create and link unique class nodes\n",
    "                for name, cls in unique_classes.items():\n",
    "                    self.kg.query(\"\"\"\n",
    "                        MERGE (class:Class {name: $name})\n",
    "                        SET class.parameters = $parameters\n",
    "                        WITH class\n",
    "                        MATCH (file:File {path: $file_path})\n",
    "                        MERGE (file)-[:DEFINES]->(class)\n",
    "                    \"\"\", {\n",
    "                        'name': name,\n",
    "                        'parameters': cls['parameters'],\n",
    "                        'file_path': file\n",
    "                    })\n",
    "\n",
    "                # Link all chunks in sequence using APOC's `NEXT` relationship\n",
    "                self.kg.query(\"\"\"\n",
    "                    MATCH (file:File {path: $file_path})-[:CONTAINS]->(chunk:Chunk)\n",
    "                    WITH chunk ORDER BY chunk.id ASC\n",
    "                    WITH collect(chunk) AS chunks\n",
    "                    CALL apoc.nodes.link(chunks, 'NEXT')\n",
    "                    RETURN count(*)\n",
    "                \"\"\", {\n",
    "                    'file_path': file\n",
    "                })\n",
    "\n",
    "    def _find_files_without_summaries(self):\n",
    "        \"\"\"\n",
    "        Find all files in the graph that do not have summaries.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of file paths that do not have summaries.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (file:File)\n",
    "        WHERE file.summary IS NULL\n",
    "        RETURN file.path AS file_path\n",
    "        \"\"\"\n",
    "        result = self.kg.query(query)\n",
    "        return [record['file_path'] for record in result]\n",
    "\n",
    "    def _find_directories_without_summaries(self):\n",
    "        \"\"\"\n",
    "        Find all directories in the graph that do not have summaries.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of directory paths that do not have summaries.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (dir:Directory)\n",
    "        WHERE dir.summary IS NULL\n",
    "        RETURN dir.path AS dir_path\n",
    "        \"\"\"\n",
    "        result = self.kg.query(query)\n",
    "        return [record['dir_path'] for record in result]\n",
    "    \n",
    "    def _summarize_file_from_chunks(self, file_path):\n",
    "        \"\"\"\n",
    "        Summarize a file based on the summaries of its chunks.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file to summarize.\n",
    "\n",
    "        Returns:\n",
    "            str: The summary of the file.\n",
    "        \"\"\"\n",
    "        # Query to get summaries of all chunks associated with the file\n",
    "        query = \"\"\"\n",
    "        MATCH (file:File {path: $file_path})-[:CONTAINS]->(chunk:Chunk)\n",
    "        RETURN chunk.summary AS chunk_summary\n",
    "        ORDER BY chunk.id ASC\n",
    "        \"\"\"\n",
    "        result = self.kg.query(query, {'file_path': file_path})\n",
    "        chunk_summaries = ['Chunk summaries'] + [record['chunk_summary'] for record in result]\n",
    "\n",
    "        # Summarize the list of chunk summaries\n",
    "        file_summary = summarize_list_of_summaries(chunk_summaries)\n",
    "\n",
    "        # Store the file summary in the graph under the \"summary\" attribute\n",
    "        self.kg.query(\"\"\"\n",
    "            MATCH (file:File {path: $file_path})\n",
    "            SET file.summary = $file_summary\n",
    "        \"\"\", {\n",
    "            'file_path': file_path,\n",
    "            'file_summary': file_summary\n",
    "        })\n",
    "\n",
    "        return file_summary\n",
    "    \n",
    "    def _summarize_directory(self, directory_path):\n",
    "        \"\"\"\n",
    "        Summarize a directory based on the summaries of its files and subdirectories.\n",
    "\n",
    "        Args:\n",
    "            directory_path (str): The path to the directory to summarize.\n",
    "\n",
    "        Returns:\n",
    "            str: The summary of the directory.\n",
    "        \"\"\"\n",
    "        # Query to get summaries of all files directly contained in the directory\n",
    "        file_query = \"\"\"\n",
    "        MATCH (dir:Directory {path: $directory_path})-[:CONTAINS]->(file:File)\n",
    "        RETURN file.summary AS file_summary\n",
    "        \"\"\"\n",
    "        file_result = self.kg.query(file_query, {'directory_path': directory_path})\n",
    "        file_summaries = [record['file_summary'] for record in file_result]\n",
    "\n",
    "        # Query to get all subdirectories directly contained in the directory\n",
    "        subdir_query = \"\"\"\n",
    "        MATCH (dir:Directory {path: $directory_path})-[:CONTAINS]->(subdir:Directory)\n",
    "        RETURN subdir.path AS subdir_path\n",
    "        \"\"\"\n",
    "        subdir_result = self.kg.query(subdir_query, {'directory_path': directory_path})\n",
    "        subdir_paths = [record['subdir_path'] for record in subdir_result]\n",
    "\n",
    "        # Recursively summarize each subdirectory if it doesn't already have a summary\n",
    "        subdir_summaries = []\n",
    "        for subdir_path in subdir_paths:\n",
    "            subdir_summary = self._summarize_directory(subdir_path)\n",
    "            subdir_summaries.append(subdir_summary)\n",
    "\n",
    "        # Combine file summaries and subdirectory summaries\n",
    "        all_summaries = ['File summaries: '] + file_summaries + ['Subdirectory summaries: ']  + subdir_summaries\n",
    "\n",
    "        # Summarize the list of all summaries (files + subdirectories)\n",
    "        directory_summary = summarize_list_of_summaries(all_summaries)\n",
    "\n",
    "        # Store the directory summary in the graph under the \"summary\" attribute\n",
    "        self.kg.query(\"\"\"\n",
    "            MATCH (dir:Directory {path: $directory_path})\n",
    "            SET dir.summary = $directory_summary\n",
    "        \"\"\", {\n",
    "            'directory_path': directory_path,\n",
    "            'directory_summary': directory_summary\n",
    "        })\n",
    "\n",
    "        return directory_summary\n",
    "\n",
    "\n",
    "    def _automate_summarization(self):\n",
    "        \"\"\"\n",
    "        Automate the summarization process for files and directories in the graph.\n",
    "        \"\"\"\n",
    "        # Summarize files without summaries\n",
    "        files_without_summaries = self._find_files_without_summaries()\n",
    "        for file_path in tqdm(files_without_summaries, desc='Summarizing files'):\n",
    "            self._summarize_file_from_chunks(file_path)\n",
    "\n",
    "        # Summarize directories without summaries\n",
    "        directories_without_summaries = self._find_directories_without_summaries()\n",
    "        for dir_path in tqdm(directories_without_summaries, 'Summarizing directories'):\n",
    "            self._summarize_directory(dir_path)\n",
    "\n",
    "\n",
    "    def create_graph(self):\n",
    "\n",
    "        self._load_dirs_and_files_to_graph()\n",
    "\n",
    "        self._enrich_graph()\n",
    "\n",
    "        self._automate_summarization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating chunks: 100%|██████████| 6/6 [00:12<00:00,  2.03s/it]\n",
      "Summarizing files: 100%|██████████| 6/6 [00:08<00:00,  1.40s/it]\n",
      "Summarizing directories: 100%|██████████| 3/3 [00:10<00:00,  3.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "codebase_graph = CodebaseGraph(\n",
    "    root_directory=\"C:\\\\Users\\\\willd\\\\Documents\\\\Git\\\\graphRag\\\\test_project\"\n",
    ")\n",
    "codebase_graph.create_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edoc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
