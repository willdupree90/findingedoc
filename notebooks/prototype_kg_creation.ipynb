{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create base graph by walking through the directory and finding files and dirs, adding them to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "from edoc.connect import connect_to_neo4j\n",
    "\n",
    "class CodebaseGraph:\n",
    "    def __init__(self, uri=\"bolt://localhost:7687\", user=None, password=None, openai_api_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the CodebaseGraph with a connection to Neo4j.\n",
    "\n",
    "        Args:\n",
    "            uri (str): The URI of the Neo4j database. Defaults to localhost.\n",
    "            user (str): Username for Neo4j. If None, loads from environment variable NEO4J_USERNAME.\n",
    "            password (str): Password for Neo4j. If None, loads from environment variable NEO4J_PASSWORD.\n",
    "            openai_api_key (str): Key needed to access OpenAI API\n",
    "        \"\"\"\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "\n",
    "        # Set up the Neo4j connection\n",
    "        self.uri = uri\n",
    "        self.NEO4J_USER = user or os.getenv(\"NEO4J_USERNAME\")\n",
    "        self.NEO4J_PASSWORD =  password or os.getenv(\"NEO4J_PASSWORD\")\n",
    "        self.OPENAI_API_KEY = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        if not self.NEO4J_USER or not self.NEO4J_PASSWORD:\n",
    "            raise ValueError(\"NEO4J_USERNAME and NEO4J_PASSWORD must be provided either as arguments or environment variables.\")\n",
    "        \n",
    "        if not self.OPENAI_API_KEY:\n",
    "            raise ValueError(\"NEO4J_USERNAME and NEO4J_PASSWORD must be provided either as arguments or environment variables.\")\n",
    "\n",
    "        self.kg = connect_to_neo4j()\n",
    "\n",
    "    def get_file_info(self, file_path):\n",
    "        \"\"\"\n",
    "        Get information about a file, including type, size, last modified date, creation date, permissions, owner, and hash.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing file information.\n",
    "        \"\"\"\n",
    "        stats = os.stat(file_path)\n",
    "        file_type = os.path.splitext(file_path)[1][1:]  # Get file extension without the dot\n",
    "        size = stats.st_size\n",
    "        last_modified = datetime.fromtimestamp(stats.st_mtime).isoformat()\n",
    "        created = datetime.fromtimestamp(stats.st_ctime).isoformat()\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"type\": file_type,\n",
    "            \"size\": size,\n",
    "            \"last_modified\": last_modified,\n",
    "            \"created\": created,\n",
    "        }\n",
    "\n",
    "    def create_graph(self, root_directory):\n",
    "        \"\"\"\n",
    "        Traverse a directory and create a graph in Neo4j representing the directory structure and file information.\n",
    "\n",
    "        Args:\n",
    "            root_directory (str): The root directory to start traversing from.\n",
    "        \"\"\"\n",
    "        for root, dirs, files in os.walk(root_directory):\n",
    "\n",
    "            dir_name = os.path.basename(root)\n",
    "\n",
    "            # Create node for the directory\n",
    "            self.kg.query(\n",
    "                \"\"\"\n",
    "                MERGE (dir:Directory {name: $dir_name, path: $path})\n",
    "                ON CREATE SET dir.created = $created, dir.last_modified = $last_modified\n",
    "                \"\"\",\n",
    "                {\n",
    "                    'dir_name':dir_name,\n",
    "                    'path':root,\n",
    "                    'created':datetime.fromtimestamp(os.stat(root).st_ctime).isoformat(),\n",
    "                    'last_modified':datetime.fromtimestamp(os.stat(root).st_mtime).isoformat(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Create nodes for subdirectories\n",
    "            for dir_name in dirs:\n",
    "                dir_path = os.path.join(root, dir_name)\n",
    "                self.kg.query(\n",
    "                    \"\"\"\n",
    "                        MERGE (subdir:Directory {name: $dir_name, path: $subdir_path})\n",
    "                        ON CREATE SET subdir.created = $created, subdir.last_modified = $last_modified\n",
    "                        WITH subdir\n",
    "                        MATCH (parent:Directory {path: $parent_path})\n",
    "                        MERGE (parent)-[:CONTAINS]->(subdir)\n",
    "                    \"\"\",\n",
    "                    {\n",
    "                        'dir_name':dir_name,\n",
    "                        'subdir_path':dir_path, \n",
    "                        'parent_path':root,\n",
    "                        'created':datetime.fromtimestamp(os.stat(dir_path).st_ctime).isoformat(),\n",
    "                        'last_modified':datetime.fromtimestamp(os.stat(dir_path).st_mtime).isoformat(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Create nodes for files\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                file_info = self.get_file_info(file_path)\n",
    "                self.kg.query(\n",
    "                    \"\"\"\n",
    "                        MERGE (file:File {name: $file_name, path: $file_path})\n",
    "                        ON CREATE SET file.type = $type, file.size = $size, file.last_modified = $last_modified, file.created = $created\n",
    "                        WITH file\n",
    "                        MATCH (parent:Directory {path: $parent_path})\n",
    "                        MERGE (parent)-[:CONTAINS]->(file)\n",
    "                    \"\"\",\n",
    "                    {\n",
    "                        'file_name':file_name, \n",
    "                        'file_path':file_path, \n",
    "                        'type':file_info['type'], \n",
    "                        'size':file_info['size'], \n",
    "                        'last_modified':file_info['last_modified'], \n",
    "                        'created':file_info['created'], \n",
    "                        'parent_path':root\n",
    "                    }\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "codebase_graph = CodebaseGraph()\n",
    "codebase_graph.create_graph(\"C:\\\\Users\\\\willd\\\\Documents\\\\Git\\\\graphRag\\\\test_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(codebase_graph.kg.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next work on file enrichments. We will need to create functions to connect to opneai api as well as functions to perform embeddings and extractoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "def create_chat_completion(messages, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Create a chat completion using the OpenAI API.\n",
    "\n",
    "    Args:\n",
    "        messages (list): A list of message dictionaries for the conversation.\n",
    "        model (str): The OpenAI model to use. Default is 'gpt-4o-mini'.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the response from the OpenAI API.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    response = client.chat.completions.create(messages=messages, model=model)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def summarize_file_chunk(chunk_text, file_name, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Summarize a chunk of text from a file using OpenAI's language model.\n",
    "\n",
    "    Args:\n",
    "        chunk_text (str): The text chunk to summarize.\n",
    "        file_name (str): The name of the file from which the chunk was extracted.\n",
    "        model (str): The OpenAI model to use. Default is 'gpt-4o-mini'.\n",
    "\n",
    "    Returns:\n",
    "        str: A brief and clear summary of the chunk.\n",
    "    \"\"\"\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"I have this text from a file named {file_name}. The text is:\\n{chunk_text}\\nPlease summarize it in simple terms. Try to keep the summary brief, but maintain clarity.\"}\n",
    "    ]\n",
    "    return create_chat_completion(messages=prompt, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = \"utils.py\"\n",
    "test_chunk = \"\"\"\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def process_file(input_path, output_path):\n",
    "    with open(input_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(content.upper())\n",
    "\n",
    "    print(f\"Processed {{input_path}} and saved results to {{output_path}}\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_summary = summarize_file_chunk(file_name=test_file_name, chunk_text=test_chunk)\n",
    "print(test_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Up next we will create embeddings from a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Generate an embedding for a given text using OpenAI's embedding model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be embedded. Newlines are replaced with spaces.\n",
    "        model (str): The OpenAI model to use. Default is 'text-embedding-3-small'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of floats representing the embedding vector of the input text.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding = get_embedding(text=test_summary)\n",
    "len(test_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onto creating a structured output to use with llms. The goal should be to take code and extract imports, functions, and classes. We use [this](https://medium.com/neo4j/enhancing-the-accuracy-of-rag-applications-with-knowledge-graphs-ad5e2ffab663) for insperation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm=ChatOpenAI(\n",
    "    model_name='gpt-4o-mini'\n",
    ")\n",
    "\n",
    "# Define a model to capture code entities\n",
    "class CodeEntities(BaseModel):\n",
    "    \"\"\"Identifying information about code entities.\"\"\"\n",
    "    \n",
    "    imports: Optional[List[dict]] = Field(\n",
    "        default=[],\n",
    "        description=\"All the import statements in the code, with the module \"\n",
    "        \"and specific entities being imported.\",\n",
    "    )\n",
    "    functions: Optional[List[dict]] = Field(\n",
    "        default=[],\n",
    "        description=\"All the function names in the code, including their parameters \"\n",
    "        \"and types if available.\",\n",
    "    )\n",
    "    classes: Optional[List[dict]] = Field(\n",
    "        default=[],\n",
    "        description=\"All the class names in the code, including their parameters \"\n",
    "        \"and types if available.\",\n",
    "    )\n",
    "\n",
    "# Modify the prompt to focus on extracting code entities\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are extracting imports, function names, and class names from the given code. \"\n",
    "            \"For imports, provide the module and specific entities being imported. \"\n",
    "            \"For functions and classes, include their parameters and types if available.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Use the given format to extract information from the following input: {code_snippet}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set up the chain to extract the structured output\n",
    "entity_chain = prompt | llm.with_structured_output(CodeEntities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_example = \"\"\"\n",
    "import java.util.List;\n",
    "import java.util.Optional;\n",
    "\n",
    "public class ExampleClass {\n",
    "    private String param1;\n",
    "    private int param2;\n",
    "\n",
    "    public ExampleClass(String param1, int param2) {\n",
    "        this.param1 = param1;\n",
    "        this.param2 = param2;\n",
    "    }\n",
    "\n",
    "    public boolean exampleMethod(List<String> arg1, Optional<Integer> arg2) {\n",
    "        return arg2.isPresent();\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_entities = entity_chain.invoke({'code_snippet': entity_example})\n",
    "\n",
    "print(\"Imports:\")\n",
    "for imp in test_entities.imports:\n",
    "    module = imp.get(\"module\", \"Unknown\")\n",
    "    imported_entities = \", \".join(imp.get(\"entities\", [])) or \"all entities\"\n",
    "    print(f\"  - {module}: {imported_entities}\")\n",
    "\n",
    "print(\"\\nFunctions:\")\n",
    "for func in test_entities.functions:\n",
    "    name = func.get(\"name\", \"Unnamed function\")\n",
    "    parameters = \", \".join([f\"{param['name']}: {param['type']}\" for param in func.get(\"parameters\", [])])\n",
    "    return_type = func.get(\"return_type\", \"Unknown\")\n",
    "    print(f\"  - {name}({parameters}) -> {return_type}\")\n",
    "\n",
    "print(\"\\nClasses:\")\n",
    "for cls in test_entities.classes:\n",
    "    name = cls.get(\"name\", \"Unnamed class\")\n",
    "    parameters = \", \".join([f\"{param['name']}: {param['type']}\" for param in cls.get(\"parameters\", [])])\n",
    "    print(f\"  - {name}({parameters})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edoc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
